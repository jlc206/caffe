I0210 21:17:11.695912 28530 caffe.cpp:184] Using GPUs 1
I0210 21:17:12.209846 28530 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 1
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:17:12.220778 28530 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:17:12.222295 28530 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:17:12.222348 28530 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:17:12.222615 28530 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:17:12.222800 28530 layer_factory.hpp:77] Creating layer mnist
I0210 21:17:12.224444 28530 net.cpp:106] Creating Layer mnist
I0210 21:17:12.224493 28530 net.cpp:411] mnist -> data
I0210 21:17:12.224637 28530 net.cpp:411] mnist -> label
I0210 21:17:12.231823 28534 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:17:12.272872 28530 data_layer.cpp:41] output data size: 64,1,28,28
I0210 21:17:12.275029 28530 net.cpp:150] Setting up mnist
I0210 21:17:12.275054 28530 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0210 21:17:12.275063 28530 net.cpp:157] Top shape: 64 (64)
I0210 21:17:12.275066 28530 net.cpp:165] Memory required for data: 200960
I0210 21:17:12.275082 28530 layer_factory.hpp:77] Creating layer conv1
I0210 21:17:12.275135 28530 net.cpp:106] Creating Layer conv1
I0210 21:17:12.275149 28530 net.cpp:454] conv1 <- data
I0210 21:17:12.275173 28530 net.cpp:411] conv1 -> conv1
I0210 21:17:12.276646 28530 net.cpp:150] Setting up conv1
I0210 21:17:12.276662 28530 net.cpp:157] Top shape: 64 20 24 24 (737280)
I0210 21:17:12.276667 28530 net.cpp:165] Memory required for data: 3150080
I0210 21:17:12.276684 28530 layer_factory.hpp:77] Creating layer pool1
I0210 21:17:12.276698 28530 net.cpp:106] Creating Layer pool1
I0210 21:17:12.276705 28530 net.cpp:454] pool1 <- conv1
I0210 21:17:12.276721 28530 net.cpp:411] pool1 -> pool1
I0210 21:17:12.276938 28530 net.cpp:150] Setting up pool1
I0210 21:17:12.276952 28530 net.cpp:157] Top shape: 64 20 12 12 (184320)
I0210 21:17:12.276957 28530 net.cpp:165] Memory required for data: 3887360
I0210 21:17:12.276963 28530 layer_factory.hpp:77] Creating layer conv2
I0210 21:17:12.276978 28530 net.cpp:106] Creating Layer conv2
I0210 21:17:12.276985 28530 net.cpp:454] conv2 <- pool1
I0210 21:17:12.276994 28530 net.cpp:411] conv2 -> conv2
I0210 21:17:12.277426 28530 net.cpp:150] Setting up conv2
I0210 21:17:12.277439 28530 net.cpp:157] Top shape: 64 50 8 8 (204800)
I0210 21:17:12.277446 28530 net.cpp:165] Memory required for data: 4706560
I0210 21:17:12.277456 28530 layer_factory.hpp:77] Creating layer pool2
I0210 21:17:12.277467 28530 net.cpp:106] Creating Layer pool2
I0210 21:17:12.277472 28530 net.cpp:454] pool2 <- conv2
I0210 21:17:12.277483 28530 net.cpp:411] pool2 -> pool2
I0210 21:17:12.277673 28530 net.cpp:150] Setting up pool2
I0210 21:17:12.277685 28530 net.cpp:157] Top shape: 64 50 4 4 (51200)
I0210 21:17:12.277690 28530 net.cpp:165] Memory required for data: 4911360
I0210 21:17:12.277695 28530 layer_factory.hpp:77] Creating layer ip1
I0210 21:17:12.277724 28530 net.cpp:106] Creating Layer ip1
I0210 21:17:12.277730 28530 net.cpp:454] ip1 <- pool2
I0210 21:17:12.277740 28530 net.cpp:411] ip1 -> ip1
I0210 21:17:12.283051 28530 net.cpp:150] Setting up ip1
I0210 21:17:12.283066 28530 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:17:12.283071 28530 net.cpp:165] Memory required for data: 5039360
I0210 21:17:12.283082 28530 layer_factory.hpp:77] Creating layer relu1
I0210 21:17:12.283094 28530 net.cpp:106] Creating Layer relu1
I0210 21:17:12.283102 28530 net.cpp:454] relu1 <- ip1
I0210 21:17:12.283110 28530 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:17:12.283123 28530 net.cpp:150] Setting up relu1
I0210 21:17:12.283131 28530 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:17:12.283136 28530 net.cpp:165] Memory required for data: 5167360
I0210 21:17:12.283141 28530 layer_factory.hpp:77] Creating layer ip2
I0210 21:17:12.283151 28530 net.cpp:106] Creating Layer ip2
I0210 21:17:12.283157 28530 net.cpp:454] ip2 <- ip1
I0210 21:17:12.283169 28530 net.cpp:411] ip2 -> ip2
I0210 21:17:12.284343 28530 net.cpp:150] Setting up ip2
I0210 21:17:12.284358 28530 net.cpp:157] Top shape: 64 10 (640)
I0210 21:17:12.284363 28530 net.cpp:165] Memory required for data: 5169920
I0210 21:17:12.284370 28530 layer_factory.hpp:77] Creating layer loss
I0210 21:17:12.284384 28530 net.cpp:106] Creating Layer loss
I0210 21:17:12.284390 28530 net.cpp:454] loss <- ip2
I0210 21:17:12.284399 28530 net.cpp:454] loss <- label
I0210 21:17:12.284411 28530 net.cpp:411] loss -> loss
I0210 21:17:12.284431 28530 layer_factory.hpp:77] Creating layer loss
I0210 21:17:12.284531 28530 net.cpp:150] Setting up loss
I0210 21:17:12.284543 28530 net.cpp:157] Top shape: (1)
I0210 21:17:12.284548 28530 net.cpp:160]     with loss weight 1
I0210 21:17:12.284586 28530 net.cpp:165] Memory required for data: 5169924
I0210 21:17:12.284602 28530 net.cpp:226] loss needs backward computation.
I0210 21:17:12.284615 28530 net.cpp:226] ip2 needs backward computation.
I0210 21:17:12.284624 28530 net.cpp:226] relu1 needs backward computation.
I0210 21:17:12.284634 28530 net.cpp:226] ip1 needs backward computation.
I0210 21:17:12.284644 28530 net.cpp:226] pool2 needs backward computation.
I0210 21:17:12.284654 28530 net.cpp:226] conv2 needs backward computation.
I0210 21:17:12.284672 28530 net.cpp:226] pool1 needs backward computation.
I0210 21:17:12.284683 28530 net.cpp:226] conv1 needs backward computation.
I0210 21:17:12.284695 28530 net.cpp:228] mnist does not need backward computation.
I0210 21:17:12.284705 28530 net.cpp:270] This network produces output loss
I0210 21:17:12.284732 28530 net.cpp:283] Network initialization done.
I0210 21:17:12.503540 28530 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:17:12.503672 28530 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:17:12.504084 28530 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:17:12.504309 28530 layer_factory.hpp:77] Creating layer mnist
I0210 21:17:12.504650 28530 net.cpp:106] Creating Layer mnist
I0210 21:17:12.504680 28530 net.cpp:411] mnist -> data
I0210 21:17:12.504916 28530 net.cpp:411] mnist -> label
I0210 21:17:12.512027 28536 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:17:12.523969 28530 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:17:12.525841 28530 net.cpp:150] Setting up mnist
I0210 21:17:12.525858 28530 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:17:12.525866 28530 net.cpp:157] Top shape: 100 (100)
I0210 21:17:12.525869 28530 net.cpp:165] Memory required for data: 314000
I0210 21:17:12.525876 28530 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:17:12.525894 28530 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:17:12.525903 28530 net.cpp:454] label_mnist_1_split <- label
I0210 21:17:12.525913 28530 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:17:12.525924 28530 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:17:12.526087 28530 net.cpp:150] Setting up label_mnist_1_split
I0210 21:17:12.526098 28530 net.cpp:157] Top shape: 100 (100)
I0210 21:17:12.526106 28530 net.cpp:157] Top shape: 100 (100)
I0210 21:17:12.526111 28530 net.cpp:165] Memory required for data: 314800
I0210 21:17:12.526118 28530 layer_factory.hpp:77] Creating layer conv1
I0210 21:17:12.526135 28530 net.cpp:106] Creating Layer conv1
I0210 21:17:12.526142 28530 net.cpp:454] conv1 <- data
I0210 21:17:12.526155 28530 net.cpp:411] conv1 -> conv1
I0210 21:17:12.526515 28530 net.cpp:150] Setting up conv1
I0210 21:17:12.526528 28530 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:17:12.526535 28530 net.cpp:165] Memory required for data: 4922800
I0210 21:17:12.526548 28530 layer_factory.hpp:77] Creating layer pool1
I0210 21:17:12.526561 28530 net.cpp:106] Creating Layer pool1
I0210 21:17:12.526571 28530 net.cpp:454] pool1 <- conv1
I0210 21:17:12.526595 28530 net.cpp:411] pool1 -> pool1
I0210 21:17:12.526772 28530 net.cpp:150] Setting up pool1
I0210 21:17:12.526784 28530 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:17:12.526789 28530 net.cpp:165] Memory required for data: 6074800
I0210 21:17:12.526795 28530 layer_factory.hpp:77] Creating layer conv2
I0210 21:17:12.526813 28530 net.cpp:106] Creating Layer conv2
I0210 21:17:12.526823 28530 net.cpp:454] conv2 <- pool1
I0210 21:17:12.526836 28530 net.cpp:411] conv2 -> conv2
I0210 21:17:12.527493 28530 net.cpp:150] Setting up conv2
I0210 21:17:12.527506 28530 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:17:12.527511 28530 net.cpp:165] Memory required for data: 7354800
I0210 21:17:12.527523 28530 layer_factory.hpp:77] Creating layer pool2
I0210 21:17:12.527537 28530 net.cpp:106] Creating Layer pool2
I0210 21:17:12.527544 28530 net.cpp:454] pool2 <- conv2
I0210 21:17:12.527552 28530 net.cpp:411] pool2 -> pool2
I0210 21:17:12.527717 28530 net.cpp:150] Setting up pool2
I0210 21:17:12.527729 28530 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:17:12.527734 28530 net.cpp:165] Memory required for data: 7674800
I0210 21:17:12.527740 28530 layer_factory.hpp:77] Creating layer ip1
I0210 21:17:12.527752 28530 net.cpp:106] Creating Layer ip1
I0210 21:17:12.527760 28530 net.cpp:454] ip1 <- pool2
I0210 21:17:12.527776 28530 net.cpp:411] ip1 -> ip1
I0210 21:17:12.533277 28530 net.cpp:150] Setting up ip1
I0210 21:17:12.533290 28530 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:17:12.533295 28530 net.cpp:165] Memory required for data: 7874800
I0210 21:17:12.533308 28530 layer_factory.hpp:77] Creating layer relu1
I0210 21:17:12.533315 28530 net.cpp:106] Creating Layer relu1
I0210 21:17:12.533321 28530 net.cpp:454] relu1 <- ip1
I0210 21:17:12.533329 28530 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:17:12.533339 28530 net.cpp:150] Setting up relu1
I0210 21:17:12.533346 28530 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:17:12.533351 28530 net.cpp:165] Memory required for data: 8074800
I0210 21:17:12.533356 28530 layer_factory.hpp:77] Creating layer ip2
I0210 21:17:12.533371 28530 net.cpp:106] Creating Layer ip2
I0210 21:17:12.533377 28530 net.cpp:454] ip2 <- ip1
I0210 21:17:12.533390 28530 net.cpp:411] ip2 -> ip2
I0210 21:17:12.533547 28530 net.cpp:150] Setting up ip2
I0210 21:17:12.533558 28530 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:12.533562 28530 net.cpp:165] Memory required for data: 8078800
I0210 21:17:12.533571 28530 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:17:12.533579 28530 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:17:12.533584 28530 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:17:12.533592 28530 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:17:12.533601 28530 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:17:12.533644 28530 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:17:12.533653 28530 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:12.533660 28530 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:12.533665 28530 net.cpp:165] Memory required for data: 8086800
I0210 21:17:12.533671 28530 layer_factory.hpp:77] Creating layer accuracy
I0210 21:17:12.533684 28530 net.cpp:106] Creating Layer accuracy
I0210 21:17:12.533691 28530 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:17:12.533697 28530 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:17:12.533709 28530 net.cpp:411] accuracy -> accuracy
I0210 21:17:12.533723 28530 net.cpp:150] Setting up accuracy
I0210 21:17:12.533731 28530 net.cpp:157] Top shape: (1)
I0210 21:17:12.533736 28530 net.cpp:165] Memory required for data: 8086804
I0210 21:17:12.533740 28530 layer_factory.hpp:77] Creating layer loss
I0210 21:17:12.533749 28530 net.cpp:106] Creating Layer loss
I0210 21:17:12.533758 28530 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:17:12.533766 28530 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:17:12.533772 28530 net.cpp:411] loss -> loss
I0210 21:17:12.533784 28530 layer_factory.hpp:77] Creating layer loss
I0210 21:17:12.533908 28530 net.cpp:150] Setting up loss
I0210 21:17:12.533932 28530 net.cpp:157] Top shape: (1)
I0210 21:17:12.533938 28530 net.cpp:160]     with loss weight 1
I0210 21:17:12.533949 28530 net.cpp:165] Memory required for data: 8086808
I0210 21:17:12.533957 28530 net.cpp:226] loss needs backward computation.
I0210 21:17:12.533962 28530 net.cpp:228] accuracy does not need backward computation.
I0210 21:17:12.533972 28530 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:17:12.533982 28530 net.cpp:226] ip2 needs backward computation.
I0210 21:17:12.533988 28530 net.cpp:226] relu1 needs backward computation.
I0210 21:17:12.533994 28530 net.cpp:226] ip1 needs backward computation.
I0210 21:17:12.533998 28530 net.cpp:226] pool2 needs backward computation.
I0210 21:17:12.534004 28530 net.cpp:226] conv2 needs backward computation.
I0210 21:17:12.534013 28530 net.cpp:226] pool1 needs backward computation.
I0210 21:17:12.534023 28530 net.cpp:226] conv1 needs backward computation.
I0210 21:17:12.534029 28530 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:17:12.534036 28530 net.cpp:228] mnist does not need backward computation.
I0210 21:17:12.534041 28530 net.cpp:270] This network produces output accuracy
I0210 21:17:12.534046 28530 net.cpp:270] This network produces output loss
I0210 21:17:12.534060 28530 net.cpp:283] Network initialization done.
I0210 21:17:12.534102 28530 solver.cpp:60] Solver scaffolding done.
I0210 21:17:12.534453 28530 caffe.cpp:212] Starting Optimization
I0210 21:17:12.534466 28530 solver.cpp:311] Solving LeNet
I0210 21:17:12.534471 28530 solver.cpp:312] Learning Rate Policy: inv
I0210 21:17:12.535156 28530 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:17:13.540608 28530 solver.cpp:432]     Test net output #0: accuracy = 0.0806
I0210 21:17:13.540685 28530 solver.cpp:432]     Test net output #1: loss = 2.33853 (* 1 = 2.33853 loss)
I0210 21:17:13.553509 28530 solver.cpp:250] Iteration 0, loss = 2.29559 Time spent communicating 0.002336
I0210 21:17:13.553534 28530 solver.cpp:267]     Train net output #0: loss = 2.29559 (* 1 = 2.29559 loss)
I0210 21:17:13.557939 28530 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:17:15.074272 28530 solver.cpp:250] Iteration 100, loss = 0.193601 Time spent communicating 0.480863
I0210 21:17:15.074321 28530 solver.cpp:267]     Train net output #0: loss = 0.193601 (* 1 = 0.193601 loss)
I0210 21:17:15.078568 28530 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:17:16.584100 28530 solver.cpp:250] Iteration 200, loss = 0.149879 Time spent communicating 0.424192
I0210 21:17:16.584122 28530 solver.cpp:267]     Train net output #0: loss = 0.149879 (* 1 = 0.149879 loss)
I0210 21:17:16.588248 28530 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:17:18.095835 28530 solver.cpp:250] Iteration 300, loss = 0.172463 Time spent communicating 0.437984
I0210 21:17:18.095883 28530 solver.cpp:267]     Train net output #0: loss = 0.172463 (* 1 = 0.172463 loss)
I0210 21:17:18.100023 28530 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:17:19.607242 28530 solver.cpp:250] Iteration 400, loss = 0.0732691 Time spent communicating 0.431296
I0210 21:17:19.607270 28530 solver.cpp:267]     Train net output #0: loss = 0.0732691 (* 1 = 0.0732691 loss)
I0210 21:17:19.611528 28530 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:17:21.106001 28530 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:17:21.136138 28530 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:17:21.166183 28530 solver.cpp:344] Iteration 500, loss = 0.0871187
I0210 21:17:21.166211 28530 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:17:22.119073 28530 solver.cpp:432]     Test net output #0: accuracy = 0.9729
I0210 21:17:22.119110 28530 solver.cpp:432]     Test net output #1: loss = 0.0881314 (* 1 = 0.0881314 loss)
I0210 21:17:22.119124 28530 solver.cpp:349] Optimization Done.
I0210 21:17:22.119134 28530 caffe.cpp:215] Optimization Done.
I0210 21:18:17.107507 28566 caffe.cpp:184] Using GPUs 1
I0210 21:18:17.662883 28566 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 1
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:18:17.672791 28566 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:18:17.674197 28566 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:18:17.674257 28566 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:18:17.674460 28566 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:18:17.674602 28566 layer_factory.hpp:77] Creating layer mnist
I0210 21:18:17.676018 28566 net.cpp:106] Creating Layer mnist
I0210 21:18:17.676064 28566 net.cpp:411] mnist -> data
I0210 21:18:17.676204 28566 net.cpp:411] mnist -> label
I0210 21:18:17.682085 28570 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:18:17.703285 28566 data_layer.cpp:41] output data size: 64,1,28,28
I0210 21:18:17.704838 28566 net.cpp:150] Setting up mnist
I0210 21:18:17.704864 28566 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0210 21:18:17.704872 28566 net.cpp:157] Top shape: 64 (64)
I0210 21:18:17.704879 28566 net.cpp:165] Memory required for data: 200960
I0210 21:18:17.704900 28566 layer_factory.hpp:77] Creating layer conv1
I0210 21:18:17.704946 28566 net.cpp:106] Creating Layer conv1
I0210 21:18:17.704958 28566 net.cpp:454] conv1 <- data
I0210 21:18:17.704984 28566 net.cpp:411] conv1 -> conv1
I0210 21:18:17.706501 28566 net.cpp:150] Setting up conv1
I0210 21:18:17.706516 28566 net.cpp:157] Top shape: 64 20 24 24 (737280)
I0210 21:18:17.706521 28566 net.cpp:165] Memory required for data: 3150080
I0210 21:18:17.706538 28566 layer_factory.hpp:77] Creating layer pool1
I0210 21:18:17.706560 28566 net.cpp:106] Creating Layer pool1
I0210 21:18:17.706567 28566 net.cpp:454] pool1 <- conv1
I0210 21:18:17.706578 28566 net.cpp:411] pool1 -> pool1
I0210 21:18:17.706840 28566 net.cpp:150] Setting up pool1
I0210 21:18:17.706856 28566 net.cpp:157] Top shape: 64 20 12 12 (184320)
I0210 21:18:17.706861 28566 net.cpp:165] Memory required for data: 3887360
I0210 21:18:17.706866 28566 layer_factory.hpp:77] Creating layer conv2
I0210 21:18:17.706883 28566 net.cpp:106] Creating Layer conv2
I0210 21:18:17.706893 28566 net.cpp:454] conv2 <- pool1
I0210 21:18:17.706908 28566 net.cpp:411] conv2 -> conv2
I0210 21:18:17.707341 28566 net.cpp:150] Setting up conv2
I0210 21:18:17.707355 28566 net.cpp:157] Top shape: 64 50 8 8 (204800)
I0210 21:18:17.707360 28566 net.cpp:165] Memory required for data: 4706560
I0210 21:18:17.707371 28566 layer_factory.hpp:77] Creating layer pool2
I0210 21:18:17.707386 28566 net.cpp:106] Creating Layer pool2
I0210 21:18:17.707392 28566 net.cpp:454] pool2 <- conv2
I0210 21:18:17.707401 28566 net.cpp:411] pool2 -> pool2
I0210 21:18:17.707557 28566 net.cpp:150] Setting up pool2
I0210 21:18:17.707569 28566 net.cpp:157] Top shape: 64 50 4 4 (51200)
I0210 21:18:17.707574 28566 net.cpp:165] Memory required for data: 4911360
I0210 21:18:17.707579 28566 layer_factory.hpp:77] Creating layer ip1
I0210 21:18:17.707604 28566 net.cpp:106] Creating Layer ip1
I0210 21:18:17.707617 28566 net.cpp:454] ip1 <- pool2
I0210 21:18:17.707633 28566 net.cpp:411] ip1 -> ip1
I0210 21:18:17.713106 28566 net.cpp:150] Setting up ip1
I0210 21:18:17.713121 28566 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:18:17.713127 28566 net.cpp:165] Memory required for data: 5039360
I0210 21:18:17.713141 28566 layer_factory.hpp:77] Creating layer relu1
I0210 21:18:17.713155 28566 net.cpp:106] Creating Layer relu1
I0210 21:18:17.713162 28566 net.cpp:454] relu1 <- ip1
I0210 21:18:17.713171 28566 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:18:17.713184 28566 net.cpp:150] Setting up relu1
I0210 21:18:17.713191 28566 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:18:17.713196 28566 net.cpp:165] Memory required for data: 5167360
I0210 21:18:17.713202 28566 layer_factory.hpp:77] Creating layer ip2
I0210 21:18:17.713217 28566 net.cpp:106] Creating Layer ip2
I0210 21:18:17.713222 28566 net.cpp:454] ip2 <- ip1
I0210 21:18:17.713230 28566 net.cpp:411] ip2 -> ip2
I0210 21:18:17.714169 28566 net.cpp:150] Setting up ip2
I0210 21:18:17.714182 28566 net.cpp:157] Top shape: 64 10 (640)
I0210 21:18:17.714190 28566 net.cpp:165] Memory required for data: 5169920
I0210 21:18:17.714205 28566 layer_factory.hpp:77] Creating layer loss
I0210 21:18:17.714222 28566 net.cpp:106] Creating Layer loss
I0210 21:18:17.714238 28566 net.cpp:454] loss <- ip2
I0210 21:18:17.714252 28566 net.cpp:454] loss <- label
I0210 21:18:17.714269 28566 net.cpp:411] loss -> loss
I0210 21:18:17.714303 28566 layer_factory.hpp:77] Creating layer loss
I0210 21:18:17.714442 28566 net.cpp:150] Setting up loss
I0210 21:18:17.714457 28566 net.cpp:157] Top shape: (1)
I0210 21:18:17.714462 28566 net.cpp:160]     with loss weight 1
I0210 21:18:17.714491 28566 net.cpp:165] Memory required for data: 5169924
I0210 21:18:17.714501 28566 net.cpp:226] loss needs backward computation.
I0210 21:18:17.714509 28566 net.cpp:226] ip2 needs backward computation.
I0210 21:18:17.714514 28566 net.cpp:226] relu1 needs backward computation.
I0210 21:18:17.714519 28566 net.cpp:226] ip1 needs backward computation.
I0210 21:18:17.714524 28566 net.cpp:226] pool2 needs backward computation.
I0210 21:18:17.714529 28566 net.cpp:226] conv2 needs backward computation.
I0210 21:18:17.714539 28566 net.cpp:226] pool1 needs backward computation.
I0210 21:18:17.714545 28566 net.cpp:226] conv1 needs backward computation.
I0210 21:18:17.714550 28566 net.cpp:228] mnist does not need backward computation.
I0210 21:18:17.714555 28566 net.cpp:270] This network produces output loss
I0210 21:18:17.714570 28566 net.cpp:283] Network initialization done.
I0210 21:18:17.715637 28566 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:18:17.715679 28566 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:18:17.715857 28566 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:18:17.715968 28566 layer_factory.hpp:77] Creating layer mnist
I0210 21:18:17.716590 28566 net.cpp:106] Creating Layer mnist
I0210 21:18:17.716606 28566 net.cpp:411] mnist -> data
I0210 21:18:17.716626 28566 net.cpp:411] mnist -> label
I0210 21:18:17.721561 28572 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:18:17.721942 28566 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:18:17.724051 28566 net.cpp:150] Setting up mnist
I0210 21:18:17.724092 28566 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:18:17.724109 28566 net.cpp:157] Top shape: 100 (100)
I0210 21:18:17.724120 28566 net.cpp:165] Memory required for data: 314000
I0210 21:18:17.724136 28566 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:18:17.724170 28566 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:18:17.724179 28566 net.cpp:454] label_mnist_1_split <- label
I0210 21:18:17.724189 28566 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:18:17.724202 28566 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:18:17.724350 28566 net.cpp:150] Setting up label_mnist_1_split
I0210 21:18:17.724362 28566 net.cpp:157] Top shape: 100 (100)
I0210 21:18:17.724370 28566 net.cpp:157] Top shape: 100 (100)
I0210 21:18:17.724375 28566 net.cpp:165] Memory required for data: 314800
I0210 21:18:17.724380 28566 layer_factory.hpp:77] Creating layer conv1
I0210 21:18:17.724406 28566 net.cpp:106] Creating Layer conv1
I0210 21:18:17.724412 28566 net.cpp:454] conv1 <- data
I0210 21:18:17.724424 28566 net.cpp:411] conv1 -> conv1
I0210 21:18:17.724853 28566 net.cpp:150] Setting up conv1
I0210 21:18:17.724881 28566 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:18:17.724901 28566 net.cpp:165] Memory required for data: 4922800
I0210 21:18:17.724931 28566 layer_factory.hpp:77] Creating layer pool1
I0210 21:18:17.724967 28566 net.cpp:106] Creating Layer pool1
I0210 21:18:17.724973 28566 net.cpp:454] pool1 <- conv1
I0210 21:18:17.725011 28566 net.cpp:411] pool1 -> pool1
I0210 21:18:17.725177 28566 net.cpp:150] Setting up pool1
I0210 21:18:17.725188 28566 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:18:17.725193 28566 net.cpp:165] Memory required for data: 6074800
I0210 21:18:17.725198 28566 layer_factory.hpp:77] Creating layer conv2
I0210 21:18:17.725217 28566 net.cpp:106] Creating Layer conv2
I0210 21:18:17.725224 28566 net.cpp:454] conv2 <- pool1
I0210 21:18:17.725234 28566 net.cpp:411] conv2 -> conv2
I0210 21:18:17.725807 28566 net.cpp:150] Setting up conv2
I0210 21:18:17.725832 28566 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:18:17.725843 28566 net.cpp:165] Memory required for data: 7354800
I0210 21:18:17.725873 28566 layer_factory.hpp:77] Creating layer pool2
I0210 21:18:17.725888 28566 net.cpp:106] Creating Layer pool2
I0210 21:18:17.725898 28566 net.cpp:454] pool2 <- conv2
I0210 21:18:17.725908 28566 net.cpp:411] pool2 -> pool2
I0210 21:18:17.726061 28566 net.cpp:150] Setting up pool2
I0210 21:18:17.726073 28566 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:18:17.726078 28566 net.cpp:165] Memory required for data: 7674800
I0210 21:18:17.726084 28566 layer_factory.hpp:77] Creating layer ip1
I0210 21:18:17.726099 28566 net.cpp:106] Creating Layer ip1
I0210 21:18:17.726106 28566 net.cpp:454] ip1 <- pool2
I0210 21:18:17.726117 28566 net.cpp:411] ip1 -> ip1
I0210 21:18:17.733355 28566 net.cpp:150] Setting up ip1
I0210 21:18:17.733369 28566 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:18:17.733374 28566 net.cpp:165] Memory required for data: 7874800
I0210 21:18:17.733386 28566 layer_factory.hpp:77] Creating layer relu1
I0210 21:18:17.733397 28566 net.cpp:106] Creating Layer relu1
I0210 21:18:17.733404 28566 net.cpp:454] relu1 <- ip1
I0210 21:18:17.733415 28566 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:18:17.733425 28566 net.cpp:150] Setting up relu1
I0210 21:18:17.733433 28566 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:18:17.733438 28566 net.cpp:165] Memory required for data: 8074800
I0210 21:18:17.733443 28566 layer_factory.hpp:77] Creating layer ip2
I0210 21:18:17.733454 28566 net.cpp:106] Creating Layer ip2
I0210 21:18:17.733459 28566 net.cpp:454] ip2 <- ip1
I0210 21:18:17.733470 28566 net.cpp:411] ip2 -> ip2
I0210 21:18:17.733624 28566 net.cpp:150] Setting up ip2
I0210 21:18:17.733636 28566 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:17.733641 28566 net.cpp:165] Memory required for data: 8078800
I0210 21:18:17.733650 28566 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:18:17.733659 28566 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:18:17.733664 28566 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:18:17.733670 28566 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:18:17.733681 28566 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:18:17.733721 28566 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:18:17.733731 28566 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:17.733737 28566 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:17.733742 28566 net.cpp:165] Memory required for data: 8086800
I0210 21:18:17.733747 28566 layer_factory.hpp:77] Creating layer accuracy
I0210 21:18:17.733767 28566 net.cpp:106] Creating Layer accuracy
I0210 21:18:17.733773 28566 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:18:17.733779 28566 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:18:17.733786 28566 net.cpp:411] accuracy -> accuracy
I0210 21:18:17.733801 28566 net.cpp:150] Setting up accuracy
I0210 21:18:17.733809 28566 net.cpp:157] Top shape: (1)
I0210 21:18:17.733814 28566 net.cpp:165] Memory required for data: 8086804
I0210 21:18:17.733819 28566 layer_factory.hpp:77] Creating layer loss
I0210 21:18:17.733830 28566 net.cpp:106] Creating Layer loss
I0210 21:18:17.733836 28566 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:18:17.733842 28566 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:18:17.733850 28566 net.cpp:411] loss -> loss
I0210 21:18:17.733860 28566 layer_factory.hpp:77] Creating layer loss
I0210 21:18:17.733994 28566 net.cpp:150] Setting up loss
I0210 21:18:17.734006 28566 net.cpp:157] Top shape: (1)
I0210 21:18:17.734012 28566 net.cpp:160]     with loss weight 1
I0210 21:18:17.734030 28566 net.cpp:165] Memory required for data: 8086808
I0210 21:18:17.734035 28566 net.cpp:226] loss needs backward computation.
I0210 21:18:17.734042 28566 net.cpp:228] accuracy does not need backward computation.
I0210 21:18:17.734048 28566 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:18:17.734055 28566 net.cpp:226] ip2 needs backward computation.
I0210 21:18:17.734060 28566 net.cpp:226] relu1 needs backward computation.
I0210 21:18:17.734064 28566 net.cpp:226] ip1 needs backward computation.
I0210 21:18:17.734071 28566 net.cpp:226] pool2 needs backward computation.
I0210 21:18:17.734081 28566 net.cpp:226] conv2 needs backward computation.
I0210 21:18:17.734089 28566 net.cpp:226] pool1 needs backward computation.
I0210 21:18:17.734094 28566 net.cpp:226] conv1 needs backward computation.
I0210 21:18:17.734100 28566 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:18:17.734107 28566 net.cpp:228] mnist does not need backward computation.
I0210 21:18:17.734112 28566 net.cpp:270] This network produces output accuracy
I0210 21:18:17.734117 28566 net.cpp:270] This network produces output loss
I0210 21:18:17.734139 28566 net.cpp:283] Network initialization done.
I0210 21:18:17.734208 28566 solver.cpp:60] Solver scaffolding done.
I0210 21:18:17.734498 28566 caffe.cpp:212] Starting Optimization
I0210 21:18:17.734508 28566 solver.cpp:311] Solving LeNet
I0210 21:18:17.734511 28566 solver.cpp:312] Learning Rate Policy: inv
I0210 21:18:17.735064 28566 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:18:18.710870 28566 solver.cpp:432]     Test net output #0: accuracy = 0.091
I0210 21:18:18.710924 28566 solver.cpp:432]     Test net output #1: loss = 2.37939 (* 1 = 2.37939 loss)
I0210 21:18:18.723580 28566 solver.cpp:250] Iteration 0, loss = 2.40601 Time spent communicating 0.00304
I0210 21:18:18.723604 28566 solver.cpp:267]     Train net output #0: loss = 2.40601 (* 1 = 2.40601 loss)
I0210 21:18:18.727622 28566 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:18:20.242652 28566 solver.cpp:250] Iteration 100, loss = 0.215063 Time spent communicating 0.444768
I0210 21:18:20.242686 28566 solver.cpp:267]     Train net output #0: loss = 0.215063 (* 1 = 0.215063 loss)
I0210 21:18:20.247054 28566 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:18:21.757596 28566 solver.cpp:250] Iteration 200, loss = 0.130853 Time spent communicating 0.43072
I0210 21:18:21.757627 28566 solver.cpp:267]     Train net output #0: loss = 0.130853 (* 1 = 0.130853 loss)
I0210 21:18:21.761752 28566 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:18:23.266767 28566 solver.cpp:250] Iteration 300, loss = 0.185694 Time spent communicating 0.431264
I0210 21:18:23.266793 28566 solver.cpp:267]     Train net output #0: loss = 0.185694 (* 1 = 0.185694 loss)
I0210 21:18:23.271489 28566 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:18:24.774930 28566 solver.cpp:250] Iteration 400, loss = 0.0460879 Time spent communicating 0.444576
I0210 21:18:24.774956 28566 solver.cpp:267]     Train net output #0: loss = 0.0460879 (* 1 = 0.0460879 loss)
I0210 21:18:24.779603 28566 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:18:26.273934 28566 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:18:26.304775 28566 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:18:26.334878 28566 solver.cpp:344] Iteration 500, loss = 0.0954321
I0210 21:18:26.334913 28566 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:18:27.290926 28566 solver.cpp:432]     Test net output #0: accuracy = 0.9742
I0210 21:18:27.290966 28566 solver.cpp:432]     Test net output #1: loss = 0.0845693 (* 1 = 0.0845693 loss)
I0210 21:18:27.290973 28566 solver.cpp:349] Optimization Done.
I0210 21:18:27.290977 28566 caffe.cpp:215] Optimization Done.
