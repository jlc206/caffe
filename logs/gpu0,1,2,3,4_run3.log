I0211 09:40:59.534418  4762 caffe.cpp:184] Using GPUs 0, 1, 2, 3, 4
I0211 09:41:04.320703  4762 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I0211 09:41:04.330555  4762 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0211 09:41:04.332023  4762 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0211 09:41:04.332051  4762 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0211 09:41:04.332201  4762 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 26
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:41:04.332288  4762 layer_factory.hpp:77] Creating layer mnist
I0211 09:41:04.333225  4762 net.cpp:106] Creating Layer mnist
I0211 09:41:04.333245  4762 net.cpp:411] mnist -> data
I0211 09:41:04.333281  4762 net.cpp:411] mnist -> label
I0211 09:41:04.339107  4766 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0211 09:41:04.358397  4762 data_layer.cpp:41] output data size: 26,1,28,28
I0211 09:41:04.359627  4762 net.cpp:150] Setting up mnist
I0211 09:41:04.359649  4762 net.cpp:157] Top shape: 26 1 28 28 (20384)
I0211 09:41:04.359658  4762 net.cpp:157] Top shape: 26 (26)
I0211 09:41:04.359663  4762 net.cpp:165] Memory required for data: 81640
I0211 09:41:04.359678  4762 layer_factory.hpp:77] Creating layer conv1
I0211 09:41:04.359707  4762 net.cpp:106] Creating Layer conv1
I0211 09:41:04.359719  4762 net.cpp:454] conv1 <- data
I0211 09:41:04.359737  4762 net.cpp:411] conv1 -> conv1
I0211 09:41:04.361035  4762 net.cpp:150] Setting up conv1
I0211 09:41:04.361050  4762 net.cpp:157] Top shape: 26 20 24 24 (299520)
I0211 09:41:04.361055  4762 net.cpp:165] Memory required for data: 1279720
I0211 09:41:04.361071  4762 layer_factory.hpp:77] Creating layer pool1
I0211 09:41:04.361088  4762 net.cpp:106] Creating Layer pool1
I0211 09:41:04.361094  4762 net.cpp:454] pool1 <- conv1
I0211 09:41:04.361104  4762 net.cpp:411] pool1 -> pool1
I0211 09:41:04.361310  4762 net.cpp:150] Setting up pool1
I0211 09:41:04.361322  4762 net.cpp:157] Top shape: 26 20 12 12 (74880)
I0211 09:41:04.361327  4762 net.cpp:165] Memory required for data: 1579240
I0211 09:41:04.361333  4762 layer_factory.hpp:77] Creating layer conv2
I0211 09:41:04.361348  4762 net.cpp:106] Creating Layer conv2
I0211 09:41:04.361354  4762 net.cpp:454] conv2 <- pool1
I0211 09:41:04.361363  4762 net.cpp:411] conv2 -> conv2
I0211 09:41:04.361786  4762 net.cpp:150] Setting up conv2
I0211 09:41:04.361800  4762 net.cpp:157] Top shape: 26 50 8 8 (83200)
I0211 09:41:04.361804  4762 net.cpp:165] Memory required for data: 1912040
I0211 09:41:04.361816  4762 layer_factory.hpp:77] Creating layer pool2
I0211 09:41:04.361826  4762 net.cpp:106] Creating Layer pool2
I0211 09:41:04.361832  4762 net.cpp:454] pool2 <- conv2
I0211 09:41:04.361845  4762 net.cpp:411] pool2 -> pool2
I0211 09:41:04.361999  4762 net.cpp:150] Setting up pool2
I0211 09:41:04.362010  4762 net.cpp:157] Top shape: 26 50 4 4 (20800)
I0211 09:41:04.362015  4762 net.cpp:165] Memory required for data: 1995240
I0211 09:41:04.362021  4762 layer_factory.hpp:77] Creating layer ip1
I0211 09:41:04.362040  4762 net.cpp:106] Creating Layer ip1
I0211 09:41:04.362046  4762 net.cpp:454] ip1 <- pool2
I0211 09:41:04.362056  4762 net.cpp:411] ip1 -> ip1
I0211 09:41:04.364377  4767 blocking_queue.cpp:50] Waiting for data
I0211 09:41:04.366564  4762 net.cpp:150] Setting up ip1
I0211 09:41:04.366580  4762 net.cpp:157] Top shape: 26 500 (13000)
I0211 09:41:04.366585  4762 net.cpp:165] Memory required for data: 2047240
I0211 09:41:04.366595  4762 layer_factory.hpp:77] Creating layer relu1
I0211 09:41:04.366605  4762 net.cpp:106] Creating Layer relu1
I0211 09:41:04.366610  4762 net.cpp:454] relu1 <- ip1
I0211 09:41:04.366619  4762 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:41:04.366632  4762 net.cpp:150] Setting up relu1
I0211 09:41:04.366641  4762 net.cpp:157] Top shape: 26 500 (13000)
I0211 09:41:04.366646  4762 net.cpp:165] Memory required for data: 2099240
I0211 09:41:04.366650  4762 layer_factory.hpp:77] Creating layer ip2
I0211 09:41:04.366659  4762 net.cpp:106] Creating Layer ip2
I0211 09:41:04.366665  4762 net.cpp:454] ip2 <- ip1
I0211 09:41:04.366673  4762 net.cpp:411] ip2 -> ip2
I0211 09:41:04.367543  4762 net.cpp:150] Setting up ip2
I0211 09:41:04.367557  4762 net.cpp:157] Top shape: 26 10 (260)
I0211 09:41:04.367561  4762 net.cpp:165] Memory required for data: 2100280
I0211 09:41:04.367569  4762 layer_factory.hpp:77] Creating layer loss
I0211 09:41:04.367583  4762 net.cpp:106] Creating Layer loss
I0211 09:41:04.367589  4762 net.cpp:454] loss <- ip2
I0211 09:41:04.367596  4762 net.cpp:454] loss <- label
I0211 09:41:04.367605  4762 net.cpp:411] loss -> loss
I0211 09:41:04.367625  4762 layer_factory.hpp:77] Creating layer loss
I0211 09:41:04.367723  4762 net.cpp:150] Setting up loss
I0211 09:41:04.367733  4762 net.cpp:157] Top shape: (1)
I0211 09:41:04.367739  4762 net.cpp:160]     with loss weight 1
I0211 09:41:04.367769  4762 net.cpp:165] Memory required for data: 2100284
I0211 09:41:04.367776  4762 net.cpp:226] loss needs backward computation.
I0211 09:41:04.367784  4762 net.cpp:226] ip2 needs backward computation.
I0211 09:41:04.367789  4762 net.cpp:226] relu1 needs backward computation.
I0211 09:41:04.367794  4762 net.cpp:226] ip1 needs backward computation.
I0211 09:41:04.367800  4762 net.cpp:226] pool2 needs backward computation.
I0211 09:41:04.367805  4762 net.cpp:226] conv2 needs backward computation.
I0211 09:41:04.367813  4762 net.cpp:226] pool1 needs backward computation.
I0211 09:41:04.367820  4762 net.cpp:226] conv1 needs backward computation.
I0211 09:41:04.367825  4762 net.cpp:228] mnist does not need backward computation.
I0211 09:41:04.367830  4762 net.cpp:270] This network produces output loss
I0211 09:41:04.367847  4762 net.cpp:283] Network initialization done.
I0211 09:41:04.369105  4762 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0211 09:41:04.369148  4762 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0211 09:41:04.369320  4762 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:41:04.369418  4762 layer_factory.hpp:77] Creating layer mnist
I0211 09:41:04.369556  4762 net.cpp:106] Creating Layer mnist
I0211 09:41:04.369570  4762 net.cpp:411] mnist -> data
I0211 09:41:04.369582  4762 net.cpp:411] mnist -> label
I0211 09:41:04.375031  4768 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0211 09:41:04.375458  4762 data_layer.cpp:41] output data size: 100,1,28,28
I0211 09:41:04.377881  4762 net.cpp:150] Setting up mnist
I0211 09:41:04.377913  4762 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0211 09:41:04.377924  4762 net.cpp:157] Top shape: 100 (100)
I0211 09:41:04.377930  4762 net.cpp:165] Memory required for data: 314000
I0211 09:41:04.377939  4762 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0211 09:41:04.377957  4762 net.cpp:106] Creating Layer label_mnist_1_split
I0211 09:41:04.377967  4762 net.cpp:454] label_mnist_1_split <- label
I0211 09:41:04.377985  4762 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0211 09:41:04.378005  4762 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0211 09:41:04.378173  4762 net.cpp:150] Setting up label_mnist_1_split
I0211 09:41:04.378190  4762 net.cpp:157] Top shape: 100 (100)
I0211 09:41:04.378201  4762 net.cpp:157] Top shape: 100 (100)
I0211 09:41:04.378209  4762 net.cpp:165] Memory required for data: 314800
I0211 09:41:04.378218  4762 layer_factory.hpp:77] Creating layer conv1
I0211 09:41:04.378250  4762 net.cpp:106] Creating Layer conv1
I0211 09:41:04.378262  4762 net.cpp:454] conv1 <- data
I0211 09:41:04.378276  4762 net.cpp:411] conv1 -> conv1
I0211 09:41:04.378654  4762 net.cpp:150] Setting up conv1
I0211 09:41:04.378667  4762 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0211 09:41:04.378672  4762 net.cpp:165] Memory required for data: 4922800
I0211 09:41:04.378684  4762 layer_factory.hpp:77] Creating layer pool1
I0211 09:41:04.378695  4762 net.cpp:106] Creating Layer pool1
I0211 09:41:04.378717  4762 net.cpp:454] pool1 <- conv1
I0211 09:41:04.378729  4762 net.cpp:411] pool1 -> pool1
I0211 09:41:04.378885  4762 net.cpp:150] Setting up pool1
I0211 09:41:04.378901  4762 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0211 09:41:04.378906  4762 net.cpp:165] Memory required for data: 6074800
I0211 09:41:04.378912  4762 layer_factory.hpp:77] Creating layer conv2
I0211 09:41:04.378927  4762 net.cpp:106] Creating Layer conv2
I0211 09:41:04.378934  4762 net.cpp:454] conv2 <- pool1
I0211 09:41:04.378943  4762 net.cpp:411] conv2 -> conv2
I0211 09:41:04.379385  4762 net.cpp:150] Setting up conv2
I0211 09:41:04.379397  4762 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0211 09:41:04.379402  4762 net.cpp:165] Memory required for data: 7354800
I0211 09:41:04.379415  4762 layer_factory.hpp:77] Creating layer pool2
I0211 09:41:04.379423  4762 net.cpp:106] Creating Layer pool2
I0211 09:41:04.379429  4762 net.cpp:454] pool2 <- conv2
I0211 09:41:04.379441  4762 net.cpp:411] pool2 -> pool2
I0211 09:41:04.379590  4762 net.cpp:150] Setting up pool2
I0211 09:41:04.379601  4762 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0211 09:41:04.379607  4762 net.cpp:165] Memory required for data: 7674800
I0211 09:41:04.379611  4762 layer_factory.hpp:77] Creating layer ip1
I0211 09:41:04.379624  4762 net.cpp:106] Creating Layer ip1
I0211 09:41:04.379631  4762 net.cpp:454] ip1 <- pool2
I0211 09:41:04.379639  4762 net.cpp:411] ip1 -> ip1
I0211 09:41:04.385799  4762 net.cpp:150] Setting up ip1
I0211 09:41:04.385823  4762 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:41:04.385833  4762 net.cpp:165] Memory required for data: 7874800
I0211 09:41:04.385849  4762 layer_factory.hpp:77] Creating layer relu1
I0211 09:41:04.385869  4762 net.cpp:106] Creating Layer relu1
I0211 09:41:04.385879  4762 net.cpp:454] relu1 <- ip1
I0211 09:41:04.385897  4762 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:41:04.385913  4762 net.cpp:150] Setting up relu1
I0211 09:41:04.385926  4762 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:41:04.385932  4762 net.cpp:165] Memory required for data: 8074800
I0211 09:41:04.385939  4762 layer_factory.hpp:77] Creating layer ip2
I0211 09:41:04.385958  4762 net.cpp:106] Creating Layer ip2
I0211 09:41:04.385967  4762 net.cpp:454] ip2 <- ip1
I0211 09:41:04.385980  4762 net.cpp:411] ip2 -> ip2
I0211 09:41:04.386217  4762 net.cpp:150] Setting up ip2
I0211 09:41:04.386234  4762 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:41:04.386242  4762 net.cpp:165] Memory required for data: 8078800
I0211 09:41:04.386255  4762 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0211 09:41:04.386268  4762 net.cpp:106] Creating Layer ip2_ip2_0_split
I0211 09:41:04.386277  4762 net.cpp:454] ip2_ip2_0_split <- ip2
I0211 09:41:04.386292  4762 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0211 09:41:04.386307  4762 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0211 09:41:04.386373  4762 net.cpp:150] Setting up ip2_ip2_0_split
I0211 09:41:04.386389  4762 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:41:04.386399  4762 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:41:04.386406  4762 net.cpp:165] Memory required for data: 8086800
I0211 09:41:04.386415  4762 layer_factory.hpp:77] Creating layer accuracy
I0211 09:41:04.386433  4762 net.cpp:106] Creating Layer accuracy
I0211 09:41:04.386442  4762 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0211 09:41:04.386453  4762 net.cpp:454] accuracy <- label_mnist_1_split_0
I0211 09:41:04.386469  4762 net.cpp:411] accuracy -> accuracy
I0211 09:41:04.386490  4762 net.cpp:150] Setting up accuracy
I0211 09:41:04.386502  4762 net.cpp:157] Top shape: (1)
I0211 09:41:04.386510  4762 net.cpp:165] Memory required for data: 8086804
I0211 09:41:04.386518  4762 layer_factory.hpp:77] Creating layer loss
I0211 09:41:04.386530  4762 net.cpp:106] Creating Layer loss
I0211 09:41:04.386539  4762 net.cpp:454] loss <- ip2_ip2_0_split_1
I0211 09:41:04.386550  4762 net.cpp:454] loss <- label_mnist_1_split_1
I0211 09:41:04.386565  4762 net.cpp:411] loss -> loss
I0211 09:41:04.386605  4762 layer_factory.hpp:77] Creating layer loss
I0211 09:41:04.386750  4762 net.cpp:150] Setting up loss
I0211 09:41:04.386766  4762 net.cpp:157] Top shape: (1)
I0211 09:41:04.386775  4762 net.cpp:160]     with loss weight 1
I0211 09:41:04.386788  4762 net.cpp:165] Memory required for data: 8086808
I0211 09:41:04.386798  4762 net.cpp:226] loss needs backward computation.
I0211 09:41:04.386807  4762 net.cpp:228] accuracy does not need backward computation.
I0211 09:41:04.386817  4762 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0211 09:41:04.386826  4762 net.cpp:226] ip2 needs backward computation.
I0211 09:41:04.386834  4762 net.cpp:226] relu1 needs backward computation.
I0211 09:41:04.386842  4762 net.cpp:226] ip1 needs backward computation.
I0211 09:41:04.386850  4762 net.cpp:226] pool2 needs backward computation.
I0211 09:41:04.386857  4762 net.cpp:226] conv2 needs backward computation.
I0211 09:41:04.386865  4762 net.cpp:226] pool1 needs backward computation.
I0211 09:41:04.386874  4762 net.cpp:226] conv1 needs backward computation.
I0211 09:41:04.386881  4762 net.cpp:228] label_mnist_1_split does not need backward computation.
I0211 09:41:04.386904  4762 net.cpp:228] mnist does not need backward computation.
I0211 09:41:04.386914  4762 net.cpp:270] This network produces output accuracy
I0211 09:41:04.386921  4762 net.cpp:270] This network produces output loss
I0211 09:41:04.386957  4762 net.cpp:283] Network initialization done.
I0211 09:41:04.386998  4762 solver.cpp:60] Solver scaffolding done.
I0211 09:41:04.443688  4762 parallel.cpp:405] GPUs pairs 0:1, 2:3, 0:2, 0:4
I0211 09:41:04.659430  4762 data_layer.cpp:41] output data size: 26,1,28,28
I0211 09:41:04.989032  4762 data_layer.cpp:41] output data size: 26,1,28,28
I0211 09:41:05.387403  4762 data_layer.cpp:41] output data size: 26,1,28,28
I0211 09:41:05.657630  4762 parallel.cpp:234] GPU 4 does not have p2p access to GPU 0
I0211 09:41:05.905498  4762 data_layer.cpp:41] output data size: 26,1,28,28
I0211 09:41:06.144850  4762 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0211 09:41:06.146131  4762 solver.cpp:311] Solving LeNet
I0211 09:41:06.146154  4762 solver.cpp:312] Learning Rate Policy: inv
I0211 09:41:06.146494  4762 solver.cpp:364] Iteration 0, Testing net (#0)
I0211 09:41:07.171545  4762 solver.cpp:432]     Test net output #0: accuracy = 0.0696
I0211 09:41:07.171602  4762 solver.cpp:432]     Test net output #1: loss = 2.45195 (* 1 = 2.45195 loss)
I0211 09:41:07.186923  4762 solver.cpp:250] Iteration 0, loss = 2.4694 Time spent communicating 0.664704
I0211 09:41:07.186962  4762 solver.cpp:267]     Train net output #0: loss = 2.4694 (* 1 = 2.4694 loss)
I0211 09:41:07.195993  4762 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0211 09:41:08.209761  4762 solver.cpp:250] Iteration 100, loss = 0.329911 Time spent communicating 136.656
I0211 09:41:08.209800  4762 solver.cpp:267]     Train net output #0: loss = 0.329911 (* 1 = 0.329911 loss)
I0211 09:41:08.211091  4762 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0211 09:41:09.209837  4762 solver.cpp:250] Iteration 200, loss = 0.061432 Time spent communicating 120.547
I0211 09:41:09.209878  4762 solver.cpp:267]     Train net output #0: loss = 0.0614322 (* 1 = 0.0614322 loss)
I0211 09:41:09.211206  4762 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0211 09:41:10.210347  4762 solver.cpp:250] Iteration 300, loss = 0.0525518 Time spent communicating 125.602
I0211 09:41:10.210391  4762 solver.cpp:267]     Train net output #0: loss = 0.052552 (* 1 = 0.052552 loss)
I0211 09:41:10.211385  4762 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0211 09:41:11.209681  4762 solver.cpp:250] Iteration 400, loss = 0.0956708 Time spent communicating 117.334
I0211 09:41:11.209722  4762 solver.cpp:267]     Train net output #0: loss = 0.095671 (* 1 = 0.095671 loss)
I0211 09:41:11.210721  4762 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0211 09:41:12.224750  4762 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0211 09:41:12.256925  4762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0211 09:41:12.284977  4762 solver.cpp:344] Iteration 500, loss = 0.101602
I0211 09:41:12.285004  4762 solver.cpp:364] Iteration 500, Testing net (#0)
I0211 09:41:13.237094  4762 solver.cpp:432]     Test net output #0: accuracy = 0.9776
I0211 09:41:13.237124  4762 solver.cpp:432]     Test net output #1: loss = 0.0724011 (* 1 = 0.0724011 loss)
I0211 09:41:13.237133  4762 solver.cpp:349] Optimization Done.
I0211 09:41:13.237221  4762 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0211 09:41:13.257840  4762 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0211 09:41:13.275748  4762 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0211 09:41:13.294283  4762 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0211 09:41:13.310072  4762 parallel.cpp:256] IN DESTRUCTOR AND I'M 0
I0211 09:41:13.310515  4762 caffe.cpp:215] Optimization Done.
