I0211 09:43:28.562849  4869 caffe.cpp:184] Using GPUs 0, 1, 2
I0211 09:43:33.357424  4869 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I0211 09:43:33.368470  4869 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0211 09:43:33.370328  4869 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0211 09:43:33.370389  4869 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0211 09:43:33.370672  4869 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 43
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:43:33.370846  4869 layer_factory.hpp:77] Creating layer mnist
I0211 09:43:33.371964  4869 net.cpp:106] Creating Layer mnist
I0211 09:43:33.371985  4869 net.cpp:411] mnist -> data
I0211 09:43:33.372021  4869 net.cpp:411] mnist -> label
I0211 09:43:33.377913  4873 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0211 09:43:33.399171  4869 data_layer.cpp:41] output data size: 43,1,28,28
I0211 09:43:33.400420  4869 net.cpp:150] Setting up mnist
I0211 09:43:33.400442  4869 net.cpp:157] Top shape: 43 1 28 28 (33712)
I0211 09:43:33.400450  4869 net.cpp:157] Top shape: 43 (43)
I0211 09:43:33.400455  4869 net.cpp:165] Memory required for data: 135020
I0211 09:43:33.400468  4869 layer_factory.hpp:77] Creating layer conv1
I0211 09:43:33.400499  4869 net.cpp:106] Creating Layer conv1
I0211 09:43:33.400511  4869 net.cpp:454] conv1 <- data
I0211 09:43:33.400528  4869 net.cpp:411] conv1 -> conv1
I0211 09:43:33.401785  4869 net.cpp:150] Setting up conv1
I0211 09:43:33.401800  4869 net.cpp:157] Top shape: 43 20 24 24 (495360)
I0211 09:43:33.401805  4869 net.cpp:165] Memory required for data: 2116460
I0211 09:43:33.401823  4869 layer_factory.hpp:77] Creating layer pool1
I0211 09:43:33.401840  4869 net.cpp:106] Creating Layer pool1
I0211 09:43:33.401847  4869 net.cpp:454] pool1 <- conv1
I0211 09:43:33.401857  4869 net.cpp:411] pool1 -> pool1
I0211 09:43:33.402060  4869 net.cpp:150] Setting up pool1
I0211 09:43:33.402072  4869 net.cpp:157] Top shape: 43 20 12 12 (123840)
I0211 09:43:33.402078  4869 net.cpp:165] Memory required for data: 2611820
I0211 09:43:33.402083  4869 layer_factory.hpp:77] Creating layer conv2
I0211 09:43:33.402102  4869 net.cpp:106] Creating Layer conv2
I0211 09:43:33.402109  4869 net.cpp:454] conv2 <- pool1
I0211 09:43:33.402118  4869 net.cpp:411] conv2 -> conv2
I0211 09:43:33.402536  4869 net.cpp:150] Setting up conv2
I0211 09:43:33.402549  4869 net.cpp:157] Top shape: 43 50 8 8 (137600)
I0211 09:43:33.402554  4869 net.cpp:165] Memory required for data: 3162220
I0211 09:43:33.402565  4869 layer_factory.hpp:77] Creating layer pool2
I0211 09:43:33.402578  4869 net.cpp:106] Creating Layer pool2
I0211 09:43:33.402585  4869 net.cpp:454] pool2 <- conv2
I0211 09:43:33.402596  4869 net.cpp:411] pool2 -> pool2
I0211 09:43:33.402750  4869 net.cpp:150] Setting up pool2
I0211 09:43:33.402760  4869 net.cpp:157] Top shape: 43 50 4 4 (34400)
I0211 09:43:33.402766  4869 net.cpp:165] Memory required for data: 3299820
I0211 09:43:33.402771  4869 layer_factory.hpp:77] Creating layer ip1
I0211 09:43:33.402787  4869 net.cpp:106] Creating Layer ip1
I0211 09:43:33.402793  4869 net.cpp:454] ip1 <- pool2
I0211 09:43:33.402804  4869 net.cpp:411] ip1 -> ip1
I0211 09:43:33.404393  4874 blocking_queue.cpp:50] Waiting for data
I0211 09:43:33.407336  4869 net.cpp:150] Setting up ip1
I0211 09:43:33.407354  4869 net.cpp:157] Top shape: 43 500 (21500)
I0211 09:43:33.407359  4869 net.cpp:165] Memory required for data: 3385820
I0211 09:43:33.407371  4869 layer_factory.hpp:77] Creating layer relu1
I0211 09:43:33.407380  4869 net.cpp:106] Creating Layer relu1
I0211 09:43:33.407387  4869 net.cpp:454] relu1 <- ip1
I0211 09:43:33.407393  4869 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:43:33.407405  4869 net.cpp:150] Setting up relu1
I0211 09:43:33.407413  4869 net.cpp:157] Top shape: 43 500 (21500)
I0211 09:43:33.407418  4869 net.cpp:165] Memory required for data: 3471820
I0211 09:43:33.407425  4869 layer_factory.hpp:77] Creating layer ip2
I0211 09:43:33.407438  4869 net.cpp:106] Creating Layer ip2
I0211 09:43:33.407444  4869 net.cpp:454] ip2 <- ip1
I0211 09:43:33.407455  4869 net.cpp:411] ip2 -> ip2
I0211 09:43:33.408337  4869 net.cpp:150] Setting up ip2
I0211 09:43:33.408351  4869 net.cpp:157] Top shape: 43 10 (430)
I0211 09:43:33.408359  4869 net.cpp:165] Memory required for data: 3473540
I0211 09:43:33.408366  4869 layer_factory.hpp:77] Creating layer loss
I0211 09:43:33.408380  4869 net.cpp:106] Creating Layer loss
I0211 09:43:33.408385  4869 net.cpp:454] loss <- ip2
I0211 09:43:33.408391  4869 net.cpp:454] loss <- label
I0211 09:43:33.408401  4869 net.cpp:411] loss -> loss
I0211 09:43:33.408421  4869 layer_factory.hpp:77] Creating layer loss
I0211 09:43:33.408519  4869 net.cpp:150] Setting up loss
I0211 09:43:33.408530  4869 net.cpp:157] Top shape: (1)
I0211 09:43:33.408535  4869 net.cpp:160]     with loss weight 1
I0211 09:43:33.408563  4869 net.cpp:165] Memory required for data: 3473544
I0211 09:43:33.408571  4869 net.cpp:226] loss needs backward computation.
I0211 09:43:33.408578  4869 net.cpp:226] ip2 needs backward computation.
I0211 09:43:33.408584  4869 net.cpp:226] relu1 needs backward computation.
I0211 09:43:33.408588  4869 net.cpp:226] ip1 needs backward computation.
I0211 09:43:33.408594  4869 net.cpp:226] pool2 needs backward computation.
I0211 09:43:33.408599  4869 net.cpp:226] conv2 needs backward computation.
I0211 09:43:33.408608  4869 net.cpp:226] pool1 needs backward computation.
I0211 09:43:33.408614  4869 net.cpp:226] conv1 needs backward computation.
I0211 09:43:33.408627  4869 net.cpp:228] mnist does not need backward computation.
I0211 09:43:33.408632  4869 net.cpp:270] This network produces output loss
I0211 09:43:33.408648  4869 net.cpp:283] Network initialization done.
I0211 09:43:33.409797  4869 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0211 09:43:33.409842  4869 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0211 09:43:33.410013  4869 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:43:33.410115  4869 layer_factory.hpp:77] Creating layer mnist
I0211 09:43:33.410751  4869 net.cpp:106] Creating Layer mnist
I0211 09:43:33.410764  4869 net.cpp:411] mnist -> data
I0211 09:43:33.410776  4869 net.cpp:411] mnist -> label
I0211 09:43:33.414773  4875 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0211 09:43:33.415112  4869 data_layer.cpp:41] output data size: 100,1,28,28
I0211 09:43:33.416751  4869 net.cpp:150] Setting up mnist
I0211 09:43:33.416767  4869 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0211 09:43:33.416774  4869 net.cpp:157] Top shape: 100 (100)
I0211 09:43:33.416780  4869 net.cpp:165] Memory required for data: 314000
I0211 09:43:33.416787  4869 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0211 09:43:33.416798  4869 net.cpp:106] Creating Layer label_mnist_1_split
I0211 09:43:33.416805  4869 net.cpp:454] label_mnist_1_split <- label
I0211 09:43:33.416813  4869 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0211 09:43:33.416826  4869 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0211 09:43:33.416883  4869 net.cpp:150] Setting up label_mnist_1_split
I0211 09:43:33.416893  4869 net.cpp:157] Top shape: 100 (100)
I0211 09:43:33.416900  4869 net.cpp:157] Top shape: 100 (100)
I0211 09:43:33.416906  4869 net.cpp:165] Memory required for data: 314800
I0211 09:43:33.416913  4869 layer_factory.hpp:77] Creating layer conv1
I0211 09:43:33.416929  4869 net.cpp:106] Creating Layer conv1
I0211 09:43:33.416935  4869 net.cpp:454] conv1 <- data
I0211 09:43:33.416944  4869 net.cpp:411] conv1 -> conv1
I0211 09:43:33.417438  4869 net.cpp:150] Setting up conv1
I0211 09:43:33.417450  4869 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0211 09:43:33.417456  4869 net.cpp:165] Memory required for data: 4922800
I0211 09:43:33.417469  4869 layer_factory.hpp:77] Creating layer pool1
I0211 09:43:33.417498  4869 net.cpp:106] Creating Layer pool1
I0211 09:43:33.417505  4869 net.cpp:454] pool1 <- conv1
I0211 09:43:33.417516  4869 net.cpp:411] pool1 -> pool1
I0211 09:43:33.417677  4869 net.cpp:150] Setting up pool1
I0211 09:43:33.417690  4869 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0211 09:43:33.417695  4869 net.cpp:165] Memory required for data: 6074800
I0211 09:43:33.417701  4869 layer_factory.hpp:77] Creating layer conv2
I0211 09:43:33.417716  4869 net.cpp:106] Creating Layer conv2
I0211 09:43:33.417721  4869 net.cpp:454] conv2 <- pool1
I0211 09:43:33.417729  4869 net.cpp:411] conv2 -> conv2
I0211 09:43:33.418166  4869 net.cpp:150] Setting up conv2
I0211 09:43:33.418179  4869 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0211 09:43:33.418184  4869 net.cpp:165] Memory required for data: 7354800
I0211 09:43:33.418197  4869 layer_factory.hpp:77] Creating layer pool2
I0211 09:43:33.418210  4869 net.cpp:106] Creating Layer pool2
I0211 09:43:33.418216  4869 net.cpp:454] pool2 <- conv2
I0211 09:43:33.418225  4869 net.cpp:411] pool2 -> pool2
I0211 09:43:33.418372  4869 net.cpp:150] Setting up pool2
I0211 09:43:33.418383  4869 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0211 09:43:33.418388  4869 net.cpp:165] Memory required for data: 7674800
I0211 09:43:33.418395  4869 layer_factory.hpp:77] Creating layer ip1
I0211 09:43:33.418406  4869 net.cpp:106] Creating Layer ip1
I0211 09:43:33.418412  4869 net.cpp:454] ip1 <- pool2
I0211 09:43:33.418421  4869 net.cpp:411] ip1 -> ip1
I0211 09:43:33.423079  4869 net.cpp:150] Setting up ip1
I0211 09:43:33.423099  4869 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:43:33.423106  4869 net.cpp:165] Memory required for data: 7874800
I0211 09:43:33.423118  4869 layer_factory.hpp:77] Creating layer relu1
I0211 09:43:33.423128  4869 net.cpp:106] Creating Layer relu1
I0211 09:43:33.423135  4869 net.cpp:454] relu1 <- ip1
I0211 09:43:33.423143  4869 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:43:33.423153  4869 net.cpp:150] Setting up relu1
I0211 09:43:33.423161  4869 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:43:33.423166  4869 net.cpp:165] Memory required for data: 8074800
I0211 09:43:33.423172  4869 layer_factory.hpp:77] Creating layer ip2
I0211 09:43:33.423187  4869 net.cpp:106] Creating Layer ip2
I0211 09:43:33.423192  4869 net.cpp:454] ip2 <- ip1
I0211 09:43:33.423202  4869 net.cpp:411] ip2 -> ip2
I0211 09:43:33.423358  4869 net.cpp:150] Setting up ip2
I0211 09:43:33.423370  4869 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:43:33.423377  4869 net.cpp:165] Memory required for data: 8078800
I0211 09:43:33.423385  4869 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0211 09:43:33.423395  4869 net.cpp:106] Creating Layer ip2_ip2_0_split
I0211 09:43:33.423401  4869 net.cpp:454] ip2_ip2_0_split <- ip2
I0211 09:43:33.423410  4869 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0211 09:43:33.423419  4869 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0211 09:43:33.423473  4869 net.cpp:150] Setting up ip2_ip2_0_split
I0211 09:43:33.423482  4869 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:43:33.423490  4869 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:43:33.423496  4869 net.cpp:165] Memory required for data: 8086800
I0211 09:43:33.423501  4869 layer_factory.hpp:77] Creating layer accuracy
I0211 09:43:33.423513  4869 net.cpp:106] Creating Layer accuracy
I0211 09:43:33.423521  4869 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0211 09:43:33.423527  4869 net.cpp:454] accuracy <- label_mnist_1_split_0
I0211 09:43:33.423537  4869 net.cpp:411] accuracy -> accuracy
I0211 09:43:33.423552  4869 net.cpp:150] Setting up accuracy
I0211 09:43:33.423560  4869 net.cpp:157] Top shape: (1)
I0211 09:43:33.423564  4869 net.cpp:165] Memory required for data: 8086804
I0211 09:43:33.423569  4869 layer_factory.hpp:77] Creating layer loss
I0211 09:43:33.423576  4869 net.cpp:106] Creating Layer loss
I0211 09:43:33.423583  4869 net.cpp:454] loss <- ip2_ip2_0_split_1
I0211 09:43:33.423589  4869 net.cpp:454] loss <- label_mnist_1_split_1
I0211 09:43:33.423599  4869 net.cpp:411] loss -> loss
I0211 09:43:33.423632  4869 layer_factory.hpp:77] Creating layer loss
I0211 09:43:33.423727  4869 net.cpp:150] Setting up loss
I0211 09:43:33.423743  4869 net.cpp:157] Top shape: (1)
I0211 09:43:33.423748  4869 net.cpp:160]     with loss weight 1
I0211 09:43:33.423758  4869 net.cpp:165] Memory required for data: 8086808
I0211 09:43:33.423764  4869 net.cpp:226] loss needs backward computation.
I0211 09:43:33.423770  4869 net.cpp:228] accuracy does not need backward computation.
I0211 09:43:33.423777  4869 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0211 09:43:33.423784  4869 net.cpp:226] ip2 needs backward computation.
I0211 09:43:33.423789  4869 net.cpp:226] relu1 needs backward computation.
I0211 09:43:33.423794  4869 net.cpp:226] ip1 needs backward computation.
I0211 09:43:33.423800  4869 net.cpp:226] pool2 needs backward computation.
I0211 09:43:33.423806  4869 net.cpp:226] conv2 needs backward computation.
I0211 09:43:33.423812  4869 net.cpp:226] pool1 needs backward computation.
I0211 09:43:33.423817  4869 net.cpp:226] conv1 needs backward computation.
I0211 09:43:33.423825  4869 net.cpp:228] label_mnist_1_split does not need backward computation.
I0211 09:43:33.423831  4869 net.cpp:228] mnist does not need backward computation.
I0211 09:43:33.423837  4869 net.cpp:270] This network produces output accuracy
I0211 09:43:33.423843  4869 net.cpp:270] This network produces output loss
I0211 09:43:33.423857  4869 net.cpp:283] Network initialization done.
I0211 09:43:33.423898  4869 solver.cpp:60] Solver scaffolding done.
I0211 09:43:33.436558  4869 parallel.cpp:405] GPUs pairs 0:1, 0:2
I0211 09:43:33.659059  4869 data_layer.cpp:41] output data size: 43,1,28,28
I0211 09:43:34.032064  4869 data_layer.cpp:41] output data size: 43,1,28,28
I0211 09:43:34.186297  4869 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0211 09:43:34.187189  4869 solver.cpp:311] Solving LeNet
I0211 09:43:34.187201  4869 solver.cpp:312] Learning Rate Policy: inv
I0211 09:43:34.187305  4869 solver.cpp:364] Iteration 0, Testing net (#0)
I0211 09:43:35.244966  4869 solver.cpp:432]     Test net output #0: accuracy = 0.1095
I0211 09:43:35.245008  4869 solver.cpp:432]     Test net output #1: loss = 2.35149 (* 1 = 2.35149 loss)
I0211 09:43:35.258471  4869 solver.cpp:250] Iteration 0, loss = 2.36701 Time spent communicating 0.36448
I0211 09:43:35.258494  4869 solver.cpp:267]     Train net output #0: loss = 2.36701 (* 1 = 2.36701 loss)
I0211 09:43:35.266252  4869 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0211 09:43:36.518491  4869 solver.cpp:250] Iteration 100, loss = 0.631255 Time spent communicating 92.4683
I0211 09:43:36.518524  4869 solver.cpp:267]     Train net output #0: loss = 0.631255 (* 1 = 0.631255 loss)
I0211 09:43:36.521582  4869 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0211 09:43:37.677084  4869 solver.cpp:250] Iteration 200, loss = 0.302781 Time spent communicating 106.015
I0211 09:43:37.677117  4869 solver.cpp:267]     Train net output #0: loss = 0.302781 (* 1 = 0.302781 loss)
I0211 09:43:37.680053  4869 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0211 09:43:38.827792  4869 solver.cpp:250] Iteration 300, loss = 0.242886 Time spent communicating 87.7502
I0211 09:43:38.827824  4869 solver.cpp:267]     Train net output #0: loss = 0.242887 (* 1 = 0.242887 loss)
I0211 09:43:38.830765  4869 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0211 09:43:40.005542  4869 solver.cpp:250] Iteration 400, loss = 0.0875267 Time spent communicating 65.5368
I0211 09:43:40.005576  4869 solver.cpp:267]     Train net output #0: loss = 0.0875268 (* 1 = 0.0875268 loss)
I0211 09:43:40.008977  4869 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0211 09:43:41.171315  4869 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0211 09:43:41.205071  4869 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0211 09:43:41.233185  4869 solver.cpp:344] Iteration 500, loss = 0.020108
I0211 09:43:41.233211  4869 solver.cpp:364] Iteration 500, Testing net (#0)
I0211 09:43:42.175343  4869 solver.cpp:432]     Test net output #0: accuracy = 0.9788
I0211 09:43:42.175374  4869 solver.cpp:432]     Test net output #1: loss = 0.0708206 (* 1 = 0.0708206 loss)
I0211 09:43:42.175381  4869 solver.cpp:349] Optimization Done.
I0211 09:43:42.175422  4869 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0211 09:43:42.190362  4869 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0211 09:43:42.204612  4869 parallel.cpp:256] IN DESTRUCTOR AND I'M 0
I0211 09:43:42.205063  4869 caffe.cpp:215] Optimization Done.
