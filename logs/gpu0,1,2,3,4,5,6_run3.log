I0211 09:37:43.416478  4555 caffe.cpp:184] Using GPUs 0, 1, 2, 3, 4, 5, 6
I0211 09:37:48.264536  4555 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I0211 09:37:48.272562  4555 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0211 09:37:48.273980  4555 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0211 09:37:48.274039  4555 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0211 09:37:48.274319  4555 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 18
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:37:48.274504  4555 layer_factory.hpp:77] Creating layer mnist
I0211 09:37:48.275939  4555 net.cpp:106] Creating Layer mnist
I0211 09:37:48.275977  4555 net.cpp:411] mnist -> data
I0211 09:37:48.276043  4555 net.cpp:411] mnist -> label
I0211 09:37:48.281844  4559 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0211 09:37:48.302065  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:48.303176  4555 net.cpp:150] Setting up mnist
I0211 09:37:48.303199  4555 net.cpp:157] Top shape: 18 1 28 28 (14112)
I0211 09:37:48.303206  4555 net.cpp:157] Top shape: 18 (18)
I0211 09:37:48.303212  4555 net.cpp:165] Memory required for data: 56520
I0211 09:37:48.303225  4555 layer_factory.hpp:77] Creating layer conv1
I0211 09:37:48.303253  4555 net.cpp:106] Creating Layer conv1
I0211 09:37:48.303266  4555 net.cpp:454] conv1 <- data
I0211 09:37:48.303288  4555 net.cpp:411] conv1 -> conv1
I0211 09:37:48.304579  4555 net.cpp:150] Setting up conv1
I0211 09:37:48.304594  4555 net.cpp:157] Top shape: 18 20 24 24 (207360)
I0211 09:37:48.304600  4555 net.cpp:165] Memory required for data: 885960
I0211 09:37:48.304616  4555 layer_factory.hpp:77] Creating layer pool1
I0211 09:37:48.304632  4555 net.cpp:106] Creating Layer pool1
I0211 09:37:48.304639  4555 net.cpp:454] pool1 <- conv1
I0211 09:37:48.304648  4555 net.cpp:411] pool1 -> pool1
I0211 09:37:48.304858  4555 net.cpp:150] Setting up pool1
I0211 09:37:48.304872  4555 net.cpp:157] Top shape: 18 20 12 12 (51840)
I0211 09:37:48.304877  4555 net.cpp:165] Memory required for data: 1093320
I0211 09:37:48.304882  4555 layer_factory.hpp:77] Creating layer conv2
I0211 09:37:48.304901  4555 net.cpp:106] Creating Layer conv2
I0211 09:37:48.304908  4555 net.cpp:454] conv2 <- pool1
I0211 09:37:48.304919  4555 net.cpp:411] conv2 -> conv2
I0211 09:37:48.306280  4555 net.cpp:150] Setting up conv2
I0211 09:37:48.306295  4555 net.cpp:157] Top shape: 18 50 8 8 (57600)
I0211 09:37:48.306300  4555 net.cpp:165] Memory required for data: 1323720
I0211 09:37:48.306311  4555 layer_factory.hpp:77] Creating layer pool2
I0211 09:37:48.306321  4555 net.cpp:106] Creating Layer pool2
I0211 09:37:48.306326  4555 net.cpp:454] pool2 <- conv2
I0211 09:37:48.306336  4555 net.cpp:411] pool2 -> pool2
I0211 09:37:48.306488  4555 net.cpp:150] Setting up pool2
I0211 09:37:48.306499  4555 net.cpp:157] Top shape: 18 50 4 4 (14400)
I0211 09:37:48.306504  4555 net.cpp:165] Memory required for data: 1381320
I0211 09:37:48.306509  4555 layer_factory.hpp:77] Creating layer ip1
I0211 09:37:48.306526  4555 net.cpp:106] Creating Layer ip1
I0211 09:37:48.306534  4555 net.cpp:454] ip1 <- pool2
I0211 09:37:48.306542  4555 net.cpp:411] ip1 -> ip1
I0211 09:37:48.307876  4560 blocking_queue.cpp:50] Waiting for data
I0211 09:37:48.311048  4555 net.cpp:150] Setting up ip1
I0211 09:37:48.311065  4555 net.cpp:157] Top shape: 18 500 (9000)
I0211 09:37:48.311070  4555 net.cpp:165] Memory required for data: 1417320
I0211 09:37:48.311081  4555 layer_factory.hpp:77] Creating layer relu1
I0211 09:37:48.311091  4555 net.cpp:106] Creating Layer relu1
I0211 09:37:48.311096  4555 net.cpp:454] relu1 <- ip1
I0211 09:37:48.311106  4555 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:37:48.311118  4555 net.cpp:150] Setting up relu1
I0211 09:37:48.311126  4555 net.cpp:157] Top shape: 18 500 (9000)
I0211 09:37:48.311131  4555 net.cpp:165] Memory required for data: 1453320
I0211 09:37:48.311136  4555 layer_factory.hpp:77] Creating layer ip2
I0211 09:37:48.311146  4555 net.cpp:106] Creating Layer ip2
I0211 09:37:48.311151  4555 net.cpp:454] ip2 <- ip1
I0211 09:37:48.311162  4555 net.cpp:411] ip2 -> ip2
I0211 09:37:48.311316  4555 net.cpp:150] Setting up ip2
I0211 09:37:48.311326  4555 net.cpp:157] Top shape: 18 10 (180)
I0211 09:37:48.311332  4555 net.cpp:165] Memory required for data: 1454040
I0211 09:37:48.311341  4555 layer_factory.hpp:77] Creating layer loss
I0211 09:37:48.311354  4555 net.cpp:106] Creating Layer loss
I0211 09:37:48.311360  4555 net.cpp:454] loss <- ip2
I0211 09:37:48.311367  4555 net.cpp:454] loss <- label
I0211 09:37:48.311378  4555 net.cpp:411] loss -> loss
I0211 09:37:48.311398  4555 layer_factory.hpp:77] Creating layer loss
I0211 09:37:48.311492  4555 net.cpp:150] Setting up loss
I0211 09:37:48.311503  4555 net.cpp:157] Top shape: (1)
I0211 09:37:48.311508  4555 net.cpp:160]     with loss weight 1
I0211 09:37:48.311537  4555 net.cpp:165] Memory required for data: 1454044
I0211 09:37:48.311547  4555 net.cpp:226] loss needs backward computation.
I0211 09:37:48.311553  4555 net.cpp:226] ip2 needs backward computation.
I0211 09:37:48.311558  4555 net.cpp:226] relu1 needs backward computation.
I0211 09:37:48.311563  4555 net.cpp:226] ip1 needs backward computation.
I0211 09:37:48.311568  4555 net.cpp:226] pool2 needs backward computation.
I0211 09:37:48.311573  4555 net.cpp:226] conv2 needs backward computation.
I0211 09:37:48.311583  4555 net.cpp:226] pool1 needs backward computation.
I0211 09:37:48.311588  4555 net.cpp:226] conv1 needs backward computation.
I0211 09:37:48.311594  4555 net.cpp:228] mnist does not need backward computation.
I0211 09:37:48.311599  4555 net.cpp:270] This network produces output loss
I0211 09:37:48.311614  4555 net.cpp:283] Network initialization done.
I0211 09:37:48.312798  4555 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0211 09:37:48.312839  4555 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0211 09:37:48.313025  4555 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:37:48.313125  4555 layer_factory.hpp:77] Creating layer mnist
I0211 09:37:48.313266  4555 net.cpp:106] Creating Layer mnist
I0211 09:37:48.313279  4555 net.cpp:411] mnist -> data
I0211 09:37:48.313294  4555 net.cpp:411] mnist -> label
I0211 09:37:48.318732  4561 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0211 09:37:48.319087  4555 data_layer.cpp:41] output data size: 100,1,28,28
I0211 09:37:48.320515  4555 net.cpp:150] Setting up mnist
I0211 09:37:48.320534  4555 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0211 09:37:48.320541  4555 net.cpp:157] Top shape: 100 (100)
I0211 09:37:48.320545  4555 net.cpp:165] Memory required for data: 314000
I0211 09:37:48.320550  4555 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0211 09:37:48.320560  4555 net.cpp:106] Creating Layer label_mnist_1_split
I0211 09:37:48.320567  4555 net.cpp:454] label_mnist_1_split <- label
I0211 09:37:48.320576  4555 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0211 09:37:48.320588  4555 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0211 09:37:48.320641  4555 net.cpp:150] Setting up label_mnist_1_split
I0211 09:37:48.320652  4555 net.cpp:157] Top shape: 100 (100)
I0211 09:37:48.320658  4555 net.cpp:157] Top shape: 100 (100)
I0211 09:37:48.320663  4555 net.cpp:165] Memory required for data: 314800
I0211 09:37:48.320668  4555 layer_factory.hpp:77] Creating layer conv1
I0211 09:37:48.320683  4555 net.cpp:106] Creating Layer conv1
I0211 09:37:48.320689  4555 net.cpp:454] conv1 <- data
I0211 09:37:48.320700  4555 net.cpp:411] conv1 -> conv1
I0211 09:37:48.321205  4555 net.cpp:150] Setting up conv1
I0211 09:37:48.321218  4555 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0211 09:37:48.321224  4555 net.cpp:165] Memory required for data: 4922800
I0211 09:37:48.321238  4555 layer_factory.hpp:77] Creating layer pool1
I0211 09:37:48.321251  4555 net.cpp:106] Creating Layer pool1
I0211 09:37:48.321274  4555 net.cpp:454] pool1 <- conv1
I0211 09:37:48.321283  4555 net.cpp:411] pool1 -> pool1
I0211 09:37:48.321439  4555 net.cpp:150] Setting up pool1
I0211 09:37:48.321450  4555 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0211 09:37:48.321456  4555 net.cpp:165] Memory required for data: 6074800
I0211 09:37:48.321461  4555 layer_factory.hpp:77] Creating layer conv2
I0211 09:37:48.321480  4555 net.cpp:106] Creating Layer conv2
I0211 09:37:48.321485  4555 net.cpp:454] conv2 <- pool1
I0211 09:37:48.321494  4555 net.cpp:411] conv2 -> conv2
I0211 09:37:48.321930  4555 net.cpp:150] Setting up conv2
I0211 09:37:48.321943  4555 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0211 09:37:48.321949  4555 net.cpp:165] Memory required for data: 7354800
I0211 09:37:48.321960  4555 layer_factory.hpp:77] Creating layer pool2
I0211 09:37:48.321972  4555 net.cpp:106] Creating Layer pool2
I0211 09:37:48.321979  4555 net.cpp:454] pool2 <- conv2
I0211 09:37:48.321987  4555 net.cpp:411] pool2 -> pool2
I0211 09:37:48.322139  4555 net.cpp:150] Setting up pool2
I0211 09:37:48.322149  4555 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0211 09:37:48.322154  4555 net.cpp:165] Memory required for data: 7674800
I0211 09:37:48.322160  4555 layer_factory.hpp:77] Creating layer ip1
I0211 09:37:48.322172  4555 net.cpp:106] Creating Layer ip1
I0211 09:37:48.322178  4555 net.cpp:454] ip1 <- pool2
I0211 09:37:48.322190  4555 net.cpp:411] ip1 -> ip1
I0211 09:37:48.330355  4555 net.cpp:150] Setting up ip1
I0211 09:37:48.330384  4555 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:37:48.330394  4555 net.cpp:165] Memory required for data: 7874800
I0211 09:37:48.330417  4555 layer_factory.hpp:77] Creating layer relu1
I0211 09:37:48.330435  4555 net.cpp:106] Creating Layer relu1
I0211 09:37:48.330447  4555 net.cpp:454] relu1 <- ip1
I0211 09:37:48.330466  4555 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:37:48.330487  4555 net.cpp:150] Setting up relu1
I0211 09:37:48.330502  4555 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:37:48.330513  4555 net.cpp:165] Memory required for data: 8074800
I0211 09:37:48.330521  4555 layer_factory.hpp:77] Creating layer ip2
I0211 09:37:48.330540  4555 net.cpp:106] Creating Layer ip2
I0211 09:37:48.330552  4555 net.cpp:454] ip2 <- ip1
I0211 09:37:48.330571  4555 net.cpp:411] ip2 -> ip2
I0211 09:37:48.330874  4555 net.cpp:150] Setting up ip2
I0211 09:37:48.330906  4555 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:48.330919  4555 net.cpp:165] Memory required for data: 8078800
I0211 09:37:48.330935  4555 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0211 09:37:48.330951  4555 net.cpp:106] Creating Layer ip2_ip2_0_split
I0211 09:37:48.330963  4555 net.cpp:454] ip2_ip2_0_split <- ip2
I0211 09:37:48.330977  4555 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0211 09:37:48.330996  4555 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0211 09:37:48.331080  4555 net.cpp:150] Setting up ip2_ip2_0_split
I0211 09:37:48.331099  4555 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:48.331113  4555 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:48.331123  4555 net.cpp:165] Memory required for data: 8086800
I0211 09:37:48.331135  4555 layer_factory.hpp:77] Creating layer accuracy
I0211 09:37:48.331162  4555 net.cpp:106] Creating Layer accuracy
I0211 09:37:48.331176  4555 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0211 09:37:48.331189  4555 net.cpp:454] accuracy <- label_mnist_1_split_0
I0211 09:37:48.331205  4555 net.cpp:411] accuracy -> accuracy
I0211 09:37:48.331233  4555 net.cpp:150] Setting up accuracy
I0211 09:37:48.331248  4555 net.cpp:157] Top shape: (1)
I0211 09:37:48.331257  4555 net.cpp:165] Memory required for data: 8086804
I0211 09:37:48.331265  4555 layer_factory.hpp:77] Creating layer loss
I0211 09:37:48.331284  4555 net.cpp:106] Creating Layer loss
I0211 09:37:48.331296  4555 net.cpp:454] loss <- ip2_ip2_0_split_1
I0211 09:37:48.331310  4555 net.cpp:454] loss <- label_mnist_1_split_1
I0211 09:37:48.331326  4555 net.cpp:411] loss -> loss
I0211 09:37:48.331377  4555 layer_factory.hpp:77] Creating layer loss
I0211 09:37:48.331558  4555 net.cpp:150] Setting up loss
I0211 09:37:48.331580  4555 net.cpp:157] Top shape: (1)
I0211 09:37:48.331590  4555 net.cpp:160]     with loss weight 1
I0211 09:37:48.331609  4555 net.cpp:165] Memory required for data: 8086808
I0211 09:37:48.331620  4555 net.cpp:226] loss needs backward computation.
I0211 09:37:48.331632  4555 net.cpp:228] accuracy does not need backward computation.
I0211 09:37:48.331645  4555 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0211 09:37:48.331656  4555 net.cpp:226] ip2 needs backward computation.
I0211 09:37:48.331667  4555 net.cpp:226] relu1 needs backward computation.
I0211 09:37:48.331678  4555 net.cpp:226] ip1 needs backward computation.
I0211 09:37:48.331689  4555 net.cpp:226] pool2 needs backward computation.
I0211 09:37:48.331701  4555 net.cpp:226] conv2 needs backward computation.
I0211 09:37:48.331712  4555 net.cpp:226] pool1 needs backward computation.
I0211 09:37:48.331723  4555 net.cpp:226] conv1 needs backward computation.
I0211 09:37:48.331735  4555 net.cpp:228] label_mnist_1_split does not need backward computation.
I0211 09:37:48.331748  4555 net.cpp:228] mnist does not need backward computation.
I0211 09:37:48.331758  4555 net.cpp:270] This network produces output accuracy
I0211 09:37:48.331768  4555 net.cpp:270] This network produces output loss
I0211 09:37:48.331802  4555 net.cpp:283] Network initialization done.
I0211 09:37:48.331881  4555 solver.cpp:60] Solver scaffolding done.
I0211 09:37:48.455088  4555 parallel.cpp:405] GPUs pairs 0:1, 2:3, 4:5, 0:2, 4:6, 0:4
I0211 09:37:48.678575  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:49.017921  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:49.433910  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:49.696049  4555 parallel.cpp:234] GPU 4 does not have p2p access to GPU 0
I0211 09:37:49.949610  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:50.468930  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:51.008563  4555 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:51.284045  4555 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0211 09:37:51.285766  4555 solver.cpp:311] Solving LeNet
I0211 09:37:51.285804  4555 solver.cpp:312] Learning Rate Policy: inv
I0211 09:37:51.286311  4555 solver.cpp:364] Iteration 0, Testing net (#0)
I0211 09:37:52.314569  4555 solver.cpp:432]     Test net output #0: accuracy = 0.0755
I0211 09:37:52.314613  4555 solver.cpp:432]     Test net output #1: loss = 2.38355 (* 1 = 2.38355 loss)
I0211 09:37:52.333626  4555 solver.cpp:250] Iteration 0, loss = 2.38476 Time spent communicating 0.64864
I0211 09:37:52.333675  4555 solver.cpp:267]     Train net output #0: loss = 2.38476 (* 1 = 2.38476 loss)
I0211 09:37:52.342267  4555 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0211 09:37:53.349462  4555 solver.cpp:250] Iteration 100, loss = 0.47926 Time spent communicating 164.832
I0211 09:37:53.349500  4555 solver.cpp:267]     Train net output #0: loss = 0.47926 (* 1 = 0.47926 loss)
I0211 09:37:53.350281  4555 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0211 09:37:54.351140  4555 solver.cpp:250] Iteration 200, loss = 0.082037 Time spent communicating 179.84
I0211 09:37:54.351181  4555 solver.cpp:267]     Train net output #0: loss = 0.0820372 (* 1 = 0.0820372 loss)
I0211 09:37:54.353291  4555 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0211 09:37:55.346248  4555 solver.cpp:250] Iteration 300, loss = 0.0475029 Time spent communicating 157.776
I0211 09:37:55.346288  4555 solver.cpp:267]     Train net output #0: loss = 0.0475031 (* 1 = 0.0475031 loss)
I0211 09:37:55.347128  4555 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0211 09:37:56.344979  4555 solver.cpp:250] Iteration 400, loss = 0.126938 Time spent communicating 127.979
I0211 09:37:56.345011  4555 solver.cpp:267]     Train net output #0: loss = 0.126938 (* 1 = 0.126938 loss)
I0211 09:37:56.346209  4555 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0211 09:37:57.349251  4555 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0211 09:37:57.386325  4555 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0211 09:37:57.412600  4555 solver.cpp:344] Iteration 500, loss = 0.0805991
I0211 09:37:57.412627  4555 solver.cpp:364] Iteration 500, Testing net (#0)
I0211 09:37:58.362233  4555 solver.cpp:432]     Test net output #0: accuracy = 0.976
I0211 09:37:58.362265  4555 solver.cpp:432]     Test net output #1: loss = 0.0765637 (* 1 = 0.0765637 loss)
I0211 09:37:58.362282  4555 solver.cpp:349] Optimization Done.
I0211 09:37:58.362403  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0211 09:37:58.386936  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0211 09:37:58.409617  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 5
I0211 09:37:58.431977  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0211 09:37:58.455198  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 6
I0211 09:37:58.476716  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0211 09:37:58.497196  4555 parallel.cpp:256] IN DESTRUCTOR AND I'M 0
I0211 09:37:58.497663  4555 caffe.cpp:215] Optimization Done.
