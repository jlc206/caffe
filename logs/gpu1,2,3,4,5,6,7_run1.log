I0210 21:28:36.074534 29122 caffe.cpp:184] Using GPUs 1, 2, 3, 4, 5, 6, 7
I0210 21:28:36.586982 29122 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 1
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:28:36.598942 29122 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:28:36.600936 29122 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:28:36.600996 29122 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:28:36.601279 29122 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 18
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:28:36.601459 29122 layer_factory.hpp:77] Creating layer mnist
I0210 21:28:36.603073 29122 net.cpp:106] Creating Layer mnist
I0210 21:28:36.603121 29122 net.cpp:411] mnist -> data
I0210 21:28:36.603222 29122 net.cpp:411] mnist -> label
I0210 21:28:36.609112 29126 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:28:36.630596 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:36.631917 29122 net.cpp:150] Setting up mnist
I0210 21:28:36.631938 29122 net.cpp:157] Top shape: 18 1 28 28 (14112)
I0210 21:28:36.631947 29122 net.cpp:157] Top shape: 18 (18)
I0210 21:28:36.631952 29122 net.cpp:165] Memory required for data: 56520
I0210 21:28:36.631964 29122 layer_factory.hpp:77] Creating layer conv1
I0210 21:28:36.631999 29122 net.cpp:106] Creating Layer conv1
I0210 21:28:36.632035 29122 net.cpp:454] conv1 <- data
I0210 21:28:36.632073 29122 net.cpp:411] conv1 -> conv1
I0210 21:28:36.633642 29122 net.cpp:150] Setting up conv1
I0210 21:28:36.633658 29122 net.cpp:157] Top shape: 18 20 24 24 (207360)
I0210 21:28:36.633663 29122 net.cpp:165] Memory required for data: 885960
I0210 21:28:36.633680 29122 layer_factory.hpp:77] Creating layer pool1
I0210 21:28:36.633699 29122 net.cpp:106] Creating Layer pool1
I0210 21:28:36.633707 29122 net.cpp:454] pool1 <- conv1
I0210 21:28:36.633718 29122 net.cpp:411] pool1 -> pool1
I0210 21:28:36.633934 29122 net.cpp:150] Setting up pool1
I0210 21:28:36.633946 29122 net.cpp:157] Top shape: 18 20 12 12 (51840)
I0210 21:28:36.633951 29122 net.cpp:165] Memory required for data: 1093320
I0210 21:28:36.633956 29122 layer_factory.hpp:77] Creating layer conv2
I0210 21:28:36.633973 29122 net.cpp:106] Creating Layer conv2
I0210 21:28:36.633980 29122 net.cpp:454] conv2 <- pool1
I0210 21:28:36.633990 29122 net.cpp:411] conv2 -> conv2
I0210 21:28:36.635553 29122 net.cpp:150] Setting up conv2
I0210 21:28:36.635568 29122 net.cpp:157] Top shape: 18 50 8 8 (57600)
I0210 21:28:36.635576 29122 net.cpp:165] Memory required for data: 1323720
I0210 21:28:36.635596 29122 layer_factory.hpp:77] Creating layer pool2
I0210 21:28:36.635617 29122 net.cpp:106] Creating Layer pool2
I0210 21:28:36.635628 29122 net.cpp:454] pool2 <- conv2
I0210 21:28:36.635644 29122 net.cpp:411] pool2 -> pool2
I0210 21:28:36.635855 29122 net.cpp:150] Setting up pool2
I0210 21:28:36.635869 29122 net.cpp:157] Top shape: 18 50 4 4 (14400)
I0210 21:28:36.635874 29122 net.cpp:165] Memory required for data: 1381320
I0210 21:28:36.635880 29122 layer_factory.hpp:77] Creating layer ip1
I0210 21:28:36.635911 29122 net.cpp:106] Creating Layer ip1
I0210 21:28:36.635924 29122 net.cpp:454] ip1 <- pool2
I0210 21:28:36.635944 29122 net.cpp:411] ip1 -> ip1
I0210 21:28:36.637460 29127 blocking_queue.cpp:50] Waiting for data
I0210 21:28:36.641381 29122 net.cpp:150] Setting up ip1
I0210 21:28:36.641398 29122 net.cpp:157] Top shape: 18 500 (9000)
I0210 21:28:36.641403 29122 net.cpp:165] Memory required for data: 1417320
I0210 21:28:36.641415 29122 layer_factory.hpp:77] Creating layer relu1
I0210 21:28:36.641432 29122 net.cpp:106] Creating Layer relu1
I0210 21:28:36.641438 29122 net.cpp:454] relu1 <- ip1
I0210 21:28:36.641448 29122 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:28:36.641461 29122 net.cpp:150] Setting up relu1
I0210 21:28:36.641469 29122 net.cpp:157] Top shape: 18 500 (9000)
I0210 21:28:36.641474 29122 net.cpp:165] Memory required for data: 1453320
I0210 21:28:36.641479 29122 layer_factory.hpp:77] Creating layer ip2
I0210 21:28:36.641490 29122 net.cpp:106] Creating Layer ip2
I0210 21:28:36.641497 29122 net.cpp:454] ip2 <- ip1
I0210 21:28:36.641505 29122 net.cpp:411] ip2 -> ip2
I0210 21:28:36.641664 29122 net.cpp:150] Setting up ip2
I0210 21:28:36.641676 29122 net.cpp:157] Top shape: 18 10 (180)
I0210 21:28:36.641681 29122 net.cpp:165] Memory required for data: 1454040
I0210 21:28:36.641690 29122 layer_factory.hpp:77] Creating layer loss
I0210 21:28:36.641701 29122 net.cpp:106] Creating Layer loss
I0210 21:28:36.641707 29122 net.cpp:454] loss <- ip2
I0210 21:28:36.641713 29122 net.cpp:454] loss <- label
I0210 21:28:36.641724 29122 net.cpp:411] loss -> loss
I0210 21:28:36.641744 29122 layer_factory.hpp:77] Creating layer loss
I0210 21:28:36.643147 29122 net.cpp:150] Setting up loss
I0210 21:28:36.643162 29122 net.cpp:157] Top shape: (1)
I0210 21:28:36.643167 29122 net.cpp:160]     with loss weight 1
I0210 21:28:36.643194 29122 net.cpp:165] Memory required for data: 1454044
I0210 21:28:36.643203 29122 net.cpp:226] loss needs backward computation.
I0210 21:28:36.643210 29122 net.cpp:226] ip2 needs backward computation.
I0210 21:28:36.643216 29122 net.cpp:226] relu1 needs backward computation.
I0210 21:28:36.643223 29122 net.cpp:226] ip1 needs backward computation.
I0210 21:28:36.643226 29122 net.cpp:226] pool2 needs backward computation.
I0210 21:28:36.643231 29122 net.cpp:226] conv2 needs backward computation.
I0210 21:28:36.643241 29122 net.cpp:226] pool1 needs backward computation.
I0210 21:28:36.643246 29122 net.cpp:226] conv1 needs backward computation.
I0210 21:28:36.643254 29122 net.cpp:228] mnist does not need backward computation.
I0210 21:28:36.643259 29122 net.cpp:270] This network produces output loss
I0210 21:28:36.643273 29122 net.cpp:283] Network initialization done.
I0210 21:28:36.644481 29122 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:28:36.644520 29122 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:28:36.644693 29122 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:28:36.644805 29122 layer_factory.hpp:77] Creating layer mnist
I0210 21:28:36.644945 29122 net.cpp:106] Creating Layer mnist
I0210 21:28:36.644964 29122 net.cpp:411] mnist -> data
I0210 21:28:36.644991 29122 net.cpp:411] mnist -> label
I0210 21:28:36.649641 29128 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:28:36.649982 29122 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:28:36.651444 29122 net.cpp:150] Setting up mnist
I0210 21:28:36.651460 29122 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:28:36.651466 29122 net.cpp:157] Top shape: 100 (100)
I0210 21:28:36.651470 29122 net.cpp:165] Memory required for data: 314000
I0210 21:28:36.651476 29122 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:28:36.651485 29122 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:28:36.651490 29122 net.cpp:454] label_mnist_1_split <- label
I0210 21:28:36.651502 29122 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:28:36.651514 29122 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:28:36.651568 29122 net.cpp:150] Setting up label_mnist_1_split
I0210 21:28:36.651578 29122 net.cpp:157] Top shape: 100 (100)
I0210 21:28:36.651585 29122 net.cpp:157] Top shape: 100 (100)
I0210 21:28:36.651590 29122 net.cpp:165] Memory required for data: 314800
I0210 21:28:36.651595 29122 layer_factory.hpp:77] Creating layer conv1
I0210 21:28:36.651612 29122 net.cpp:106] Creating Layer conv1
I0210 21:28:36.651618 29122 net.cpp:454] conv1 <- data
I0210 21:28:36.651631 29122 net.cpp:411] conv1 -> conv1
I0210 21:28:36.652159 29122 net.cpp:150] Setting up conv1
I0210 21:28:36.652173 29122 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:28:36.652179 29122 net.cpp:165] Memory required for data: 4922800
I0210 21:28:36.652191 29122 layer_factory.hpp:77] Creating layer pool1
I0210 21:28:36.652204 29122 net.cpp:106] Creating Layer pool1
I0210 21:28:36.652222 29122 net.cpp:454] pool1 <- conv1
I0210 21:28:36.652233 29122 net.cpp:411] pool1 -> pool1
I0210 21:28:36.652408 29122 net.cpp:150] Setting up pool1
I0210 21:28:36.652421 29122 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:28:36.652426 29122 net.cpp:165] Memory required for data: 6074800
I0210 21:28:36.652431 29122 layer_factory.hpp:77] Creating layer conv2
I0210 21:28:36.652444 29122 net.cpp:106] Creating Layer conv2
I0210 21:28:36.652451 29122 net.cpp:454] conv2 <- pool1
I0210 21:28:36.652458 29122 net.cpp:411] conv2 -> conv2
I0210 21:28:36.652904 29122 net.cpp:150] Setting up conv2
I0210 21:28:36.652916 29122 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:28:36.652920 29122 net.cpp:165] Memory required for data: 7354800
I0210 21:28:36.652931 29122 layer_factory.hpp:77] Creating layer pool2
I0210 21:28:36.652961 29122 net.cpp:106] Creating Layer pool2
I0210 21:28:36.652973 29122 net.cpp:454] pool2 <- conv2
I0210 21:28:36.652994 29122 net.cpp:411] pool2 -> pool2
I0210 21:28:36.653316 29122 net.cpp:150] Setting up pool2
I0210 21:28:36.653340 29122 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:28:36.653352 29122 net.cpp:165] Memory required for data: 7674800
I0210 21:28:36.653362 29122 layer_factory.hpp:77] Creating layer ip1
I0210 21:28:36.653383 29122 net.cpp:106] Creating Layer ip1
I0210 21:28:36.653395 29122 net.cpp:454] ip1 <- pool2
I0210 21:28:36.653419 29122 net.cpp:411] ip1 -> ip1
I0210 21:28:36.662978 29122 net.cpp:150] Setting up ip1
I0210 21:28:36.662992 29122 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:28:36.662997 29122 net.cpp:165] Memory required for data: 7874800
I0210 21:28:36.663017 29122 layer_factory.hpp:77] Creating layer relu1
I0210 21:28:36.663029 29122 net.cpp:106] Creating Layer relu1
I0210 21:28:36.663035 29122 net.cpp:454] relu1 <- ip1
I0210 21:28:36.663044 29122 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:28:36.663054 29122 net.cpp:150] Setting up relu1
I0210 21:28:36.663061 29122 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:28:36.663066 29122 net.cpp:165] Memory required for data: 8074800
I0210 21:28:36.663071 29122 layer_factory.hpp:77] Creating layer ip2
I0210 21:28:36.663081 29122 net.cpp:106] Creating Layer ip2
I0210 21:28:36.663089 29122 net.cpp:454] ip2 <- ip1
I0210 21:28:36.663100 29122 net.cpp:411] ip2 -> ip2
I0210 21:28:36.663270 29122 net.cpp:150] Setting up ip2
I0210 21:28:36.663281 29122 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:28:36.663287 29122 net.cpp:165] Memory required for data: 8078800
I0210 21:28:36.663296 29122 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:28:36.663305 29122 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:28:36.663311 29122 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:28:36.663317 29122 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:28:36.663327 29122 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:28:36.663368 29122 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:28:36.663378 29122 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:28:36.663385 29122 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:28:36.663390 29122 net.cpp:165] Memory required for data: 8086800
I0210 21:28:36.663396 29122 layer_factory.hpp:77] Creating layer accuracy
I0210 21:28:36.663409 29122 net.cpp:106] Creating Layer accuracy
I0210 21:28:36.663415 29122 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:28:36.663422 29122 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:28:36.663430 29122 net.cpp:411] accuracy -> accuracy
I0210 21:28:36.663444 29122 net.cpp:150] Setting up accuracy
I0210 21:28:36.663452 29122 net.cpp:157] Top shape: (1)
I0210 21:28:36.663457 29122 net.cpp:165] Memory required for data: 8086804
I0210 21:28:36.663461 29122 layer_factory.hpp:77] Creating layer loss
I0210 21:28:36.663472 29122 net.cpp:106] Creating Layer loss
I0210 21:28:36.663478 29122 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:28:36.663486 29122 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:28:36.663496 29122 net.cpp:411] loss -> loss
I0210 21:28:36.663522 29122 layer_factory.hpp:77] Creating layer loss
I0210 21:28:36.663610 29122 net.cpp:150] Setting up loss
I0210 21:28:36.663620 29122 net.cpp:157] Top shape: (1)
I0210 21:28:36.663626 29122 net.cpp:160]     with loss weight 1
I0210 21:28:36.663635 29122 net.cpp:165] Memory required for data: 8086808
I0210 21:28:36.663640 29122 net.cpp:226] loss needs backward computation.
I0210 21:28:36.663645 29122 net.cpp:228] accuracy does not need backward computation.
I0210 21:28:36.663650 29122 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:28:36.663655 29122 net.cpp:226] ip2 needs backward computation.
I0210 21:28:36.663660 29122 net.cpp:226] relu1 needs backward computation.
I0210 21:28:36.663663 29122 net.cpp:226] ip1 needs backward computation.
I0210 21:28:36.663667 29122 net.cpp:226] pool2 needs backward computation.
I0210 21:28:36.663672 29122 net.cpp:226] conv2 needs backward computation.
I0210 21:28:36.663676 29122 net.cpp:226] pool1 needs backward computation.
I0210 21:28:36.663681 29122 net.cpp:226] conv1 needs backward computation.
I0210 21:28:36.663686 29122 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:28:36.663691 29122 net.cpp:228] mnist does not need backward computation.
I0210 21:28:36.663699 29122 net.cpp:270] This network produces output accuracy
I0210 21:28:36.663704 29122 net.cpp:270] This network produces output loss
I0210 21:28:36.663717 29122 net.cpp:283] Network initialization done.
I0210 21:28:36.663763 29122 solver.cpp:60] Solver scaffolding done.
I0210 21:28:36.797092 29122 parallel.cpp:405] GPUs pairs 1:2, 4:5, 6:7, 1:3, 4:6, 1:4
I0210 21:28:37.019192 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:37.348778 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:37.738225 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:37.861747 29122 parallel.cpp:234] GPU 4 does not have p2p access to GPU 1
I0210 21:28:38.147441 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:38.620385 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:39.144316 29122 data_layer.cpp:41] output data size: 18,1,28,28
I0210 21:28:39.339587 29122 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0210 21:28:39.340306 29122 solver.cpp:311] Solving LeNet
I0210 21:28:39.340333 29122 solver.cpp:312] Learning Rate Policy: inv
I0210 21:28:39.341406 29122 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:28:40.407544 29122 solver.cpp:432]     Test net output #0: accuracy = 0.0884
I0210 21:28:40.407598 29122 solver.cpp:432]     Test net output #1: loss = 2.3456 (* 1 = 2.3456 loss)
I0210 21:28:40.422830 29122 solver.cpp:250] Iteration 0, loss = 2.24763 Time spent communicating 0.718976
I0210 21:28:40.422858 29122 solver.cpp:267]     Train net output #0: loss = 2.24763 (* 1 = 2.24763 loss)
I0210 21:28:40.429041 29122 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:28:41.396219 29122 solver.cpp:250] Iteration 100, loss = 0.487371 Time spent communicating 203.097
I0210 21:28:41.396253 29122 solver.cpp:267]     Train net output #0: loss = 0.487371 (* 1 = 0.487371 loss)
I0210 21:28:41.397879 29122 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:28:42.376878 29122 solver.cpp:250] Iteration 200, loss = 0.0439019 Time spent communicating 152.46
I0210 21:28:42.376907 29122 solver.cpp:267]     Train net output #0: loss = 0.043902 (* 1 = 0.043902 loss)
I0210 21:28:42.378371 29122 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:28:43.343502 29122 solver.cpp:250] Iteration 300, loss = 0.0601211 Time spent communicating 172.547
I0210 21:28:43.343536 29122 solver.cpp:267]     Train net output #0: loss = 0.0601212 (* 1 = 0.0601212 loss)
I0210 21:28:43.345638 29122 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:28:44.316474 29122 solver.cpp:250] Iteration 400, loss = 0.160404 Time spent communicating 174.344
I0210 21:28:44.316516 29122 solver.cpp:267]     Train net output #0: loss = 0.160404 (* 1 = 0.160404 loss)
I0210 21:28:44.317570 29122 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:28:45.289841 29122 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:28:45.325152 29122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:28:45.353947 29122 solver.cpp:344] Iteration 500, loss = 0.0409363
I0210 21:28:45.353996 29122 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:28:46.300694 29122 solver.cpp:432]     Test net output #0: accuracy = 0.9771
I0210 21:28:46.300746 29122 solver.cpp:432]     Test net output #1: loss = 0.0738998 (* 1 = 0.0738998 loss)
I0210 21:28:46.300758 29122 solver.cpp:349] Optimization Done.
I0210 21:28:46.300886 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0210 21:28:46.325839 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 5
I0210 21:28:46.348475 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 7
I0210 21:28:46.371388 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0210 21:28:46.395046 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 6
I0210 21:28:46.416944 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0210 21:28:46.437296 29122 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0210 21:28:46.437793 29122 caffe.cpp:215] Optimization Done.
