I0211 09:35:03.618764  4358 caffe.cpp:184] Using GPUs 0, 1, 2, 3, 4, 5, 6, 7
I0211 09:35:08.393364  4358 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I0211 09:35:08.402060  4358 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0211 09:35:08.403403  4358 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0211 09:35:08.403460  4358 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0211 09:35:08.403746  4358 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 16
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:35:08.403975  4358 layer_factory.hpp:77] Creating layer mnist
I0211 09:35:08.405395  4358 net.cpp:106] Creating Layer mnist
I0211 09:35:08.405436  4358 net.cpp:411] mnist -> data
I0211 09:35:08.405583  4358 net.cpp:411] mnist -> label
I0211 09:35:08.412879  4362 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0211 09:35:08.452119  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:08.453250  4358 net.cpp:150] Setting up mnist
I0211 09:35:08.453274  4358 net.cpp:157] Top shape: 16 1 28 28 (12544)
I0211 09:35:08.453281  4358 net.cpp:157] Top shape: 16 (16)
I0211 09:35:08.453286  4358 net.cpp:165] Memory required for data: 50240
I0211 09:35:08.453301  4358 layer_factory.hpp:77] Creating layer conv1
I0211 09:35:08.453338  4358 net.cpp:106] Creating Layer conv1
I0211 09:35:08.453349  4358 net.cpp:454] conv1 <- data
I0211 09:35:08.453372  4358 net.cpp:411] conv1 -> conv1
I0211 09:35:08.454638  4358 net.cpp:150] Setting up conv1
I0211 09:35:08.454653  4358 net.cpp:157] Top shape: 16 20 24 24 (184320)
I0211 09:35:08.454658  4358 net.cpp:165] Memory required for data: 787520
I0211 09:35:08.454674  4358 layer_factory.hpp:77] Creating layer pool1
I0211 09:35:08.454689  4358 net.cpp:106] Creating Layer pool1
I0211 09:35:08.454695  4358 net.cpp:454] pool1 <- conv1
I0211 09:35:08.454706  4358 net.cpp:411] pool1 -> pool1
I0211 09:35:08.454916  4358 net.cpp:150] Setting up pool1
I0211 09:35:08.454929  4358 net.cpp:157] Top shape: 16 20 12 12 (46080)
I0211 09:35:08.454936  4358 net.cpp:165] Memory required for data: 971840
I0211 09:35:08.454941  4358 layer_factory.hpp:77] Creating layer conv2
I0211 09:35:08.454957  4358 net.cpp:106] Creating Layer conv2
I0211 09:35:08.454962  4358 net.cpp:454] conv2 <- pool1
I0211 09:35:08.454974  4358 net.cpp:411] conv2 -> conv2
I0211 09:35:08.456358  4358 net.cpp:150] Setting up conv2
I0211 09:35:08.456374  4358 net.cpp:157] Top shape: 16 50 8 8 (51200)
I0211 09:35:08.456379  4358 net.cpp:165] Memory required for data: 1176640
I0211 09:35:08.456390  4358 layer_factory.hpp:77] Creating layer pool2
I0211 09:35:08.456403  4358 net.cpp:106] Creating Layer pool2
I0211 09:35:08.456411  4358 net.cpp:454] pool2 <- conv2
I0211 09:35:08.456419  4358 net.cpp:411] pool2 -> pool2
I0211 09:35:08.456573  4358 net.cpp:150] Setting up pool2
I0211 09:35:08.456583  4358 net.cpp:157] Top shape: 16 50 4 4 (12800)
I0211 09:35:08.456589  4358 net.cpp:165] Memory required for data: 1227840
I0211 09:35:08.456594  4358 layer_factory.hpp:77] Creating layer ip1
I0211 09:35:08.456609  4358 net.cpp:106] Creating Layer ip1
I0211 09:35:08.456615  4358 net.cpp:454] ip1 <- pool2
I0211 09:35:08.456627  4358 net.cpp:411] ip1 -> ip1
I0211 09:35:08.457998  4363 blocking_queue.cpp:50] Waiting for data
I0211 09:35:08.461179  4358 net.cpp:150] Setting up ip1
I0211 09:35:08.461195  4358 net.cpp:157] Top shape: 16 500 (8000)
I0211 09:35:08.461200  4358 net.cpp:165] Memory required for data: 1259840
I0211 09:35:08.461212  4358 layer_factory.hpp:77] Creating layer relu1
I0211 09:35:08.461221  4358 net.cpp:106] Creating Layer relu1
I0211 09:35:08.461226  4358 net.cpp:454] relu1 <- ip1
I0211 09:35:08.461235  4358 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:35:08.461246  4358 net.cpp:150] Setting up relu1
I0211 09:35:08.461253  4358 net.cpp:157] Top shape: 16 500 (8000)
I0211 09:35:08.461258  4358 net.cpp:165] Memory required for data: 1291840
I0211 09:35:08.461263  4358 layer_factory.hpp:77] Creating layer ip2
I0211 09:35:08.461277  4358 net.cpp:106] Creating Layer ip2
I0211 09:35:08.461282  4358 net.cpp:454] ip2 <- ip1
I0211 09:35:08.461293  4358 net.cpp:411] ip2 -> ip2
I0211 09:35:08.461437  4358 net.cpp:150] Setting up ip2
I0211 09:35:08.461448  4358 net.cpp:157] Top shape: 16 10 (160)
I0211 09:35:08.461453  4358 net.cpp:165] Memory required for data: 1292480
I0211 09:35:08.461462  4358 layer_factory.hpp:77] Creating layer loss
I0211 09:35:08.461473  4358 net.cpp:106] Creating Layer loss
I0211 09:35:08.461479  4358 net.cpp:454] loss <- ip2
I0211 09:35:08.461486  4358 net.cpp:454] loss <- label
I0211 09:35:08.461495  4358 net.cpp:411] loss -> loss
I0211 09:35:08.461514  4358 layer_factory.hpp:77] Creating layer loss
I0211 09:35:08.461611  4358 net.cpp:150] Setting up loss
I0211 09:35:08.461621  4358 net.cpp:157] Top shape: (1)
I0211 09:35:08.461627  4358 net.cpp:160]     with loss weight 1
I0211 09:35:08.461663  4358 net.cpp:165] Memory required for data: 1292484
I0211 09:35:08.461673  4358 net.cpp:226] loss needs backward computation.
I0211 09:35:08.461683  4358 net.cpp:226] ip2 needs backward computation.
I0211 09:35:08.461688  4358 net.cpp:226] relu1 needs backward computation.
I0211 09:35:08.461694  4358 net.cpp:226] ip1 needs backward computation.
I0211 09:35:08.461700  4358 net.cpp:226] pool2 needs backward computation.
I0211 09:35:08.461705  4358 net.cpp:226] conv2 needs backward computation.
I0211 09:35:08.461714  4358 net.cpp:226] pool1 needs backward computation.
I0211 09:35:08.461722  4358 net.cpp:226] conv1 needs backward computation.
I0211 09:35:08.461730  4358 net.cpp:228] mnist does not need backward computation.
I0211 09:35:08.461735  4358 net.cpp:270] This network produces output loss
I0211 09:35:08.461753  4358 net.cpp:283] Network initialization done.
I0211 09:35:08.683590  4358 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0211 09:35:08.683708  4358 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0211 09:35:08.684152  4358 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:35:08.684376  4358 layer_factory.hpp:77] Creating layer mnist
I0211 09:35:08.685456  4358 net.cpp:106] Creating Layer mnist
I0211 09:35:08.685487  4358 net.cpp:411] mnist -> data
I0211 09:35:08.685530  4358 net.cpp:411] mnist -> label
I0211 09:35:08.691862  4364 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0211 09:35:08.703578  4358 data_layer.cpp:41] output data size: 100,1,28,28
I0211 09:35:08.705153  4358 net.cpp:150] Setting up mnist
I0211 09:35:08.705171  4358 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0211 09:35:08.705178  4358 net.cpp:157] Top shape: 100 (100)
I0211 09:35:08.705183  4358 net.cpp:165] Memory required for data: 314000
I0211 09:35:08.705189  4358 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0211 09:35:08.705199  4358 net.cpp:106] Creating Layer label_mnist_1_split
I0211 09:35:08.705206  4358 net.cpp:454] label_mnist_1_split <- label
I0211 09:35:08.705235  4358 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0211 09:35:08.705252  4358 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0211 09:35:08.705384  4358 net.cpp:150] Setting up label_mnist_1_split
I0211 09:35:08.705395  4358 net.cpp:157] Top shape: 100 (100)
I0211 09:35:08.705401  4358 net.cpp:157] Top shape: 100 (100)
I0211 09:35:08.705406  4358 net.cpp:165] Memory required for data: 314800
I0211 09:35:08.705412  4358 layer_factory.hpp:77] Creating layer conv1
I0211 09:35:08.705432  4358 net.cpp:106] Creating Layer conv1
I0211 09:35:08.705440  4358 net.cpp:454] conv1 <- data
I0211 09:35:08.705449  4358 net.cpp:411] conv1 -> conv1
I0211 09:35:08.705835  4358 net.cpp:150] Setting up conv1
I0211 09:35:08.705848  4358 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0211 09:35:08.705853  4358 net.cpp:165] Memory required for data: 4922800
I0211 09:35:08.705867  4358 layer_factory.hpp:77] Creating layer pool1
I0211 09:35:08.705876  4358 net.cpp:106] Creating Layer pool1
I0211 09:35:08.705905  4358 net.cpp:454] pool1 <- conv1
I0211 09:35:08.705921  4358 net.cpp:411] pool1 -> pool1
I0211 09:35:08.706087  4358 net.cpp:150] Setting up pool1
I0211 09:35:08.706099  4358 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0211 09:35:08.706105  4358 net.cpp:165] Memory required for data: 6074800
I0211 09:35:08.706110  4358 layer_factory.hpp:77] Creating layer conv2
I0211 09:35:08.706126  4358 net.cpp:106] Creating Layer conv2
I0211 09:35:08.706132  4358 net.cpp:454] conv2 <- pool1
I0211 09:35:08.706145  4358 net.cpp:411] conv2 -> conv2
I0211 09:35:08.706723  4358 net.cpp:150] Setting up conv2
I0211 09:35:08.706737  4358 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0211 09:35:08.706743  4358 net.cpp:165] Memory required for data: 7354800
I0211 09:35:08.706753  4358 layer_factory.hpp:77] Creating layer pool2
I0211 09:35:08.706763  4358 net.cpp:106] Creating Layer pool2
I0211 09:35:08.706768  4358 net.cpp:454] pool2 <- conv2
I0211 09:35:08.706778  4358 net.cpp:411] pool2 -> pool2
I0211 09:35:08.706943  4358 net.cpp:150] Setting up pool2
I0211 09:35:08.706954  4358 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0211 09:35:08.706959  4358 net.cpp:165] Memory required for data: 7674800
I0211 09:35:08.706965  4358 layer_factory.hpp:77] Creating layer ip1
I0211 09:35:08.706990  4358 net.cpp:106] Creating Layer ip1
I0211 09:35:08.706997  4358 net.cpp:454] ip1 <- pool2
I0211 09:35:08.707015  4358 net.cpp:411] ip1 -> ip1
I0211 09:35:08.711815  4358 net.cpp:150] Setting up ip1
I0211 09:35:08.711830  4358 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:35:08.711835  4358 net.cpp:165] Memory required for data: 7874800
I0211 09:35:08.711846  4358 layer_factory.hpp:77] Creating layer relu1
I0211 09:35:08.711859  4358 net.cpp:106] Creating Layer relu1
I0211 09:35:08.711865  4358 net.cpp:454] relu1 <- ip1
I0211 09:35:08.711874  4358 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:35:08.711884  4358 net.cpp:150] Setting up relu1
I0211 09:35:08.711896  4358 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:35:08.711902  4358 net.cpp:165] Memory required for data: 8074800
I0211 09:35:08.711907  4358 layer_factory.hpp:77] Creating layer ip2
I0211 09:35:08.711920  4358 net.cpp:106] Creating Layer ip2
I0211 09:35:08.711926  4358 net.cpp:454] ip2 <- ip1
I0211 09:35:08.711935  4358 net.cpp:411] ip2 -> ip2
I0211 09:35:08.712096  4358 net.cpp:150] Setting up ip2
I0211 09:35:08.712107  4358 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:35:08.712113  4358 net.cpp:165] Memory required for data: 8078800
I0211 09:35:08.712124  4358 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0211 09:35:08.712136  4358 net.cpp:106] Creating Layer ip2_ip2_0_split
I0211 09:35:08.712143  4358 net.cpp:454] ip2_ip2_0_split <- ip2
I0211 09:35:08.712152  4358 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0211 09:35:08.712162  4358 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0211 09:35:08.712203  4358 net.cpp:150] Setting up ip2_ip2_0_split
I0211 09:35:08.712213  4358 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:35:08.712220  4358 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:35:08.712226  4358 net.cpp:165] Memory required for data: 8086800
I0211 09:35:08.712234  4358 layer_factory.hpp:77] Creating layer accuracy
I0211 09:35:08.712246  4358 net.cpp:106] Creating Layer accuracy
I0211 09:35:08.712255  4358 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0211 09:35:08.712262  4358 net.cpp:454] accuracy <- label_mnist_1_split_0
I0211 09:35:08.712270  4358 net.cpp:411] accuracy -> accuracy
I0211 09:35:08.712285  4358 net.cpp:150] Setting up accuracy
I0211 09:35:08.712293  4358 net.cpp:157] Top shape: (1)
I0211 09:35:08.712298  4358 net.cpp:165] Memory required for data: 8086804
I0211 09:35:08.712304  4358 layer_factory.hpp:77] Creating layer loss
I0211 09:35:08.712313  4358 net.cpp:106] Creating Layer loss
I0211 09:35:08.712319  4358 net.cpp:454] loss <- ip2_ip2_0_split_1
I0211 09:35:08.712326  4358 net.cpp:454] loss <- label_mnist_1_split_1
I0211 09:35:08.712337  4358 net.cpp:411] loss -> loss
I0211 09:35:08.712363  4358 layer_factory.hpp:77] Creating layer loss
I0211 09:35:08.712460  4358 net.cpp:150] Setting up loss
I0211 09:35:08.712471  4358 net.cpp:157] Top shape: (1)
I0211 09:35:08.712476  4358 net.cpp:160]     with loss weight 1
I0211 09:35:08.712488  4358 net.cpp:165] Memory required for data: 8086808
I0211 09:35:08.712496  4358 net.cpp:226] loss needs backward computation.
I0211 09:35:08.712502  4358 net.cpp:228] accuracy does not need backward computation.
I0211 09:35:08.712509  4358 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0211 09:35:08.712515  4358 net.cpp:226] ip2 needs backward computation.
I0211 09:35:08.712522  4358 net.cpp:226] relu1 needs backward computation.
I0211 09:35:08.712527  4358 net.cpp:226] ip1 needs backward computation.
I0211 09:35:08.712533  4358 net.cpp:226] pool2 needs backward computation.
I0211 09:35:08.712539  4358 net.cpp:226] conv2 needs backward computation.
I0211 09:35:08.712548  4358 net.cpp:226] pool1 needs backward computation.
I0211 09:35:08.712553  4358 net.cpp:226] conv1 needs backward computation.
I0211 09:35:08.712559  4358 net.cpp:228] label_mnist_1_split does not need backward computation.
I0211 09:35:08.712565  4358 net.cpp:228] mnist does not need backward computation.
I0211 09:35:08.712571  4358 net.cpp:270] This network produces output accuracy
I0211 09:35:08.712579  4358 net.cpp:270] This network produces output loss
I0211 09:35:08.712595  4358 net.cpp:283] Network initialization done.
I0211 09:35:08.712641  4358 solver.cpp:60] Solver scaffolding done.
I0211 09:35:08.866829  4358 parallel.cpp:405] GPUs pairs 0:1, 2:3, 4:5, 6:7, 0:2, 4:6, 0:4
I0211 09:35:09.084918  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:09.422014  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:09.845010  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:10.107307  4358 parallel.cpp:234] GPU 4 does not have p2p access to GPU 0
I0211 09:35:10.351166  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:10.851894  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:11.406667  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:11.979207  4358 data_layer.cpp:41] output data size: 16,1,28,28
I0211 09:35:12.276063  4358 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0211 09:35:12.278646  4358 solver.cpp:311] Solving LeNet
I0211 09:35:12.278676  4358 solver.cpp:312] Learning Rate Policy: inv
I0211 09:35:12.279917  4358 solver.cpp:364] Iteration 0, Testing net (#0)
I0211 09:35:13.282809  4358 solver.cpp:432]     Test net output #0: accuracy = 0.1109
I0211 09:35:13.282850  4358 solver.cpp:432]     Test net output #1: loss = 2.33903 (* 1 = 2.33903 loss)
I0211 09:35:13.311636  4358 solver.cpp:250] Iteration 0, loss = 2.45292 Time spent communicating 0.645056
I0211 09:35:13.311691  4358 solver.cpp:267]     Train net output #0: loss = 2.45292 (* 1 = 2.45292 loss)
I0211 09:35:13.320708  4358 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0211 09:35:14.350369  4358 solver.cpp:250] Iteration 100, loss = 0.179827 Time spent communicating 214.882
I0211 09:35:14.350402  4358 solver.cpp:267]     Train net output #0: loss = 0.179827 (* 1 = 0.179827 loss)
I0211 09:35:14.352504  4358 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0211 09:35:15.337205  4358 solver.cpp:250] Iteration 200, loss = 0.120277 Time spent communicating 179.254
I0211 09:35:15.337241  4358 solver.cpp:267]     Train net output #0: loss = 0.120277 (* 1 = 0.120277 loss)
I0211 09:35:15.338269  4358 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0211 09:35:16.311622  4358 solver.cpp:250] Iteration 300, loss = 0.314124 Time spent communicating 200.257
I0211 09:35:16.311668  4358 solver.cpp:267]     Train net output #0: loss = 0.314124 (* 1 = 0.314124 loss)
I0211 09:35:16.312640  4358 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0211 09:35:17.304584  4358 solver.cpp:250] Iteration 400, loss = 0.638794 Time spent communicating 201.703
I0211 09:35:17.304623  4358 solver.cpp:267]     Train net output #0: loss = 0.638795 (* 1 = 0.638795 loss)
I0211 09:35:17.306066  4358 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0211 09:35:18.322628  4358 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0211 09:35:18.363984  4358 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0211 09:35:18.393352  4358 solver.cpp:344] Iteration 500, loss = 0.0128561
I0211 09:35:18.393378  4358 solver.cpp:364] Iteration 500, Testing net (#0)
I0211 09:35:19.336874  4358 solver.cpp:432]     Test net output #0: accuracy = 0.9739
I0211 09:35:19.336905  4358 solver.cpp:432]     Test net output #1: loss = 0.0803711 (* 1 = 0.0803711 loss)
I0211 09:35:19.336917  4358 solver.cpp:349] Optimization Done.
I0211 09:35:19.337040  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0211 09:35:19.364183  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0211 09:35:19.388952  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 5
I0211 09:35:19.413722  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 7
I0211 09:35:19.438127  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0211 09:35:19.463516  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 6
I0211 09:35:19.487788  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0211 09:35:19.510304  4358 parallel.cpp:256] IN DESTRUCTOR AND I'M 0
I0211 09:35:19.510747  4358 caffe.cpp:215] Optimization Done.
