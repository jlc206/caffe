I0210 21:17:22.666548 28539 caffe.cpp:184] Using GPUs 1
I0210 21:17:23.177353 28539 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 1
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:17:23.189642 28539 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:17:23.191404 28539 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:17:23.191462 28539 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:17:23.191776 28539 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:17:23.191968 28539 layer_factory.hpp:77] Creating layer mnist
I0210 21:17:23.193472 28539 net.cpp:106] Creating Layer mnist
I0210 21:17:23.193526 28539 net.cpp:411] mnist -> data
I0210 21:17:23.193600 28539 net.cpp:411] mnist -> label
I0210 21:17:23.199034 28543 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:17:23.220949 28539 data_layer.cpp:41] output data size: 64,1,28,28
I0210 21:17:23.222390 28539 net.cpp:150] Setting up mnist
I0210 21:17:23.222411 28539 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0210 21:17:23.222425 28539 net.cpp:157] Top shape: 64 (64)
I0210 21:17:23.222430 28539 net.cpp:165] Memory required for data: 200960
I0210 21:17:23.222443 28539 layer_factory.hpp:77] Creating layer conv1
I0210 21:17:23.222470 28539 net.cpp:106] Creating Layer conv1
I0210 21:17:23.222481 28539 net.cpp:454] conv1 <- data
I0210 21:17:23.222503 28539 net.cpp:411] conv1 -> conv1
I0210 21:17:23.223803 28539 net.cpp:150] Setting up conv1
I0210 21:17:23.223819 28539 net.cpp:157] Top shape: 64 20 24 24 (737280)
I0210 21:17:23.223825 28539 net.cpp:165] Memory required for data: 3150080
I0210 21:17:23.223841 28539 layer_factory.hpp:77] Creating layer pool1
I0210 21:17:23.223860 28539 net.cpp:106] Creating Layer pool1
I0210 21:17:23.223866 28539 net.cpp:454] pool1 <- conv1
I0210 21:17:23.223875 28539 net.cpp:411] pool1 -> pool1
I0210 21:17:23.224082 28539 net.cpp:150] Setting up pool1
I0210 21:17:23.224094 28539 net.cpp:157] Top shape: 64 20 12 12 (184320)
I0210 21:17:23.224100 28539 net.cpp:165] Memory required for data: 3887360
I0210 21:17:23.224107 28539 layer_factory.hpp:77] Creating layer conv2
I0210 21:17:23.224123 28539 net.cpp:106] Creating Layer conv2
I0210 21:17:23.224128 28539 net.cpp:454] conv2 <- pool1
I0210 21:17:23.224140 28539 net.cpp:411] conv2 -> conv2
I0210 21:17:23.224618 28539 net.cpp:150] Setting up conv2
I0210 21:17:23.224632 28539 net.cpp:157] Top shape: 64 50 8 8 (204800)
I0210 21:17:23.224638 28539 net.cpp:165] Memory required for data: 4706560
I0210 21:17:23.224650 28539 layer_factory.hpp:77] Creating layer pool2
I0210 21:17:23.224663 28539 net.cpp:106] Creating Layer pool2
I0210 21:17:23.224668 28539 net.cpp:454] pool2 <- conv2
I0210 21:17:23.224675 28539 net.cpp:411] pool2 -> pool2
I0210 21:17:23.224856 28539 net.cpp:150] Setting up pool2
I0210 21:17:23.224869 28539 net.cpp:157] Top shape: 64 50 4 4 (51200)
I0210 21:17:23.224874 28539 net.cpp:165] Memory required for data: 4911360
I0210 21:17:23.224879 28539 layer_factory.hpp:77] Creating layer ip1
I0210 21:17:23.224897 28539 net.cpp:106] Creating Layer ip1
I0210 21:17:23.224905 28539 net.cpp:454] ip1 <- pool2
I0210 21:17:23.224917 28539 net.cpp:411] ip1 -> ip1
I0210 21:17:23.230057 28539 net.cpp:150] Setting up ip1
I0210 21:17:23.230072 28539 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:17:23.230077 28539 net.cpp:165] Memory required for data: 5039360
I0210 21:17:23.230087 28539 layer_factory.hpp:77] Creating layer relu1
I0210 21:17:23.230096 28539 net.cpp:106] Creating Layer relu1
I0210 21:17:23.230101 28539 net.cpp:454] relu1 <- ip1
I0210 21:17:23.230108 28539 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:17:23.230120 28539 net.cpp:150] Setting up relu1
I0210 21:17:23.230129 28539 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:17:23.230134 28539 net.cpp:165] Memory required for data: 5167360
I0210 21:17:23.230139 28539 layer_factory.hpp:77] Creating layer ip2
I0210 21:17:23.230151 28539 net.cpp:106] Creating Layer ip2
I0210 21:17:23.230157 28539 net.cpp:454] ip2 <- ip1
I0210 21:17:23.230165 28539 net.cpp:411] ip2 -> ip2
I0210 21:17:23.231181 28539 net.cpp:150] Setting up ip2
I0210 21:17:23.231195 28539 net.cpp:157] Top shape: 64 10 (640)
I0210 21:17:23.231200 28539 net.cpp:165] Memory required for data: 5169920
I0210 21:17:23.231209 28539 layer_factory.hpp:77] Creating layer loss
I0210 21:17:23.231221 28539 net.cpp:106] Creating Layer loss
I0210 21:17:23.231231 28539 net.cpp:454] loss <- ip2
I0210 21:17:23.231241 28539 net.cpp:454] loss <- label
I0210 21:17:23.231254 28539 net.cpp:411] loss -> loss
I0210 21:17:23.231274 28539 layer_factory.hpp:77] Creating layer loss
I0210 21:17:23.231377 28539 net.cpp:150] Setting up loss
I0210 21:17:23.231389 28539 net.cpp:157] Top shape: (1)
I0210 21:17:23.231394 28539 net.cpp:160]     with loss weight 1
I0210 21:17:23.231421 28539 net.cpp:165] Memory required for data: 5169924
I0210 21:17:23.231429 28539 net.cpp:226] loss needs backward computation.
I0210 21:17:23.231436 28539 net.cpp:226] ip2 needs backward computation.
I0210 21:17:23.231442 28539 net.cpp:226] relu1 needs backward computation.
I0210 21:17:23.231447 28539 net.cpp:226] ip1 needs backward computation.
I0210 21:17:23.231451 28539 net.cpp:226] pool2 needs backward computation.
I0210 21:17:23.231456 28539 net.cpp:226] conv2 needs backward computation.
I0210 21:17:23.231464 28539 net.cpp:226] pool1 needs backward computation.
I0210 21:17:23.231470 28539 net.cpp:226] conv1 needs backward computation.
I0210 21:17:23.231477 28539 net.cpp:228] mnist does not need backward computation.
I0210 21:17:23.231482 28539 net.cpp:270] This network produces output loss
I0210 21:17:23.231495 28539 net.cpp:283] Network initialization done.
I0210 21:17:23.232566 28539 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:17:23.232606 28539 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:17:23.232775 28539 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:17:23.232874 28539 layer_factory.hpp:77] Creating layer mnist
I0210 21:17:23.233017 28539 net.cpp:106] Creating Layer mnist
I0210 21:17:23.233031 28539 net.cpp:411] mnist -> data
I0210 21:17:23.233044 28539 net.cpp:411] mnist -> label
I0210 21:17:23.237805 28545 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:17:23.238126 28539 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:17:23.239591 28539 net.cpp:150] Setting up mnist
I0210 21:17:23.239608 28539 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:17:23.239614 28539 net.cpp:157] Top shape: 100 (100)
I0210 21:17:23.239622 28539 net.cpp:165] Memory required for data: 314000
I0210 21:17:23.239629 28539 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:17:23.239642 28539 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:17:23.239648 28539 net.cpp:454] label_mnist_1_split <- label
I0210 21:17:23.239657 28539 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:17:23.239672 28539 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:17:23.239763 28539 net.cpp:150] Setting up label_mnist_1_split
I0210 21:17:23.239779 28539 net.cpp:157] Top shape: 100 (100)
I0210 21:17:23.239791 28539 net.cpp:157] Top shape: 100 (100)
I0210 21:17:23.239797 28539 net.cpp:165] Memory required for data: 314800
I0210 21:17:23.239802 28539 layer_factory.hpp:77] Creating layer conv1
I0210 21:17:23.239820 28539 net.cpp:106] Creating Layer conv1
I0210 21:17:23.239827 28539 net.cpp:454] conv1 <- data
I0210 21:17:23.239845 28539 net.cpp:411] conv1 -> conv1
I0210 21:17:23.240089 28539 net.cpp:150] Setting up conv1
I0210 21:17:23.240103 28539 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:17:23.240108 28539 net.cpp:165] Memory required for data: 4922800
I0210 21:17:23.240120 28539 layer_factory.hpp:77] Creating layer pool1
I0210 21:17:23.240134 28539 net.cpp:106] Creating Layer pool1
I0210 21:17:23.240139 28539 net.cpp:454] pool1 <- conv1
I0210 21:17:23.240164 28539 net.cpp:411] pool1 -> pool1
I0210 21:17:23.240320 28539 net.cpp:150] Setting up pool1
I0210 21:17:23.240334 28539 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:17:23.240339 28539 net.cpp:165] Memory required for data: 6074800
I0210 21:17:23.240345 28539 layer_factory.hpp:77] Creating layer conv2
I0210 21:17:23.240357 28539 net.cpp:106] Creating Layer conv2
I0210 21:17:23.240363 28539 net.cpp:454] conv2 <- pool1
I0210 21:17:23.240375 28539 net.cpp:411] conv2 -> conv2
I0210 21:17:23.240809 28539 net.cpp:150] Setting up conv2
I0210 21:17:23.240821 28539 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:17:23.240826 28539 net.cpp:165] Memory required for data: 7354800
I0210 21:17:23.240836 28539 layer_factory.hpp:77] Creating layer pool2
I0210 21:17:23.240854 28539 net.cpp:106] Creating Layer pool2
I0210 21:17:23.240862 28539 net.cpp:454] pool2 <- conv2
I0210 21:17:23.240870 28539 net.cpp:411] pool2 -> pool2
I0210 21:17:23.241041 28539 net.cpp:150] Setting up pool2
I0210 21:17:23.241057 28539 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:17:23.241063 28539 net.cpp:165] Memory required for data: 7674800
I0210 21:17:23.241070 28539 layer_factory.hpp:77] Creating layer ip1
I0210 21:17:23.241078 28539 net.cpp:106] Creating Layer ip1
I0210 21:17:23.241085 28539 net.cpp:454] ip1 <- pool2
I0210 21:17:23.241096 28539 net.cpp:411] ip1 -> ip1
I0210 21:17:23.247612 28539 net.cpp:150] Setting up ip1
I0210 21:17:23.247638 28539 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:17:23.247647 28539 net.cpp:165] Memory required for data: 7874800
I0210 21:17:23.247665 28539 layer_factory.hpp:77] Creating layer relu1
I0210 21:17:23.247684 28539 net.cpp:106] Creating Layer relu1
I0210 21:17:23.247694 28539 net.cpp:454] relu1 <- ip1
I0210 21:17:23.247712 28539 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:17:23.247730 28539 net.cpp:150] Setting up relu1
I0210 21:17:23.247742 28539 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:17:23.247751 28539 net.cpp:165] Memory required for data: 8074800
I0210 21:17:23.247759 28539 layer_factory.hpp:77] Creating layer ip2
I0210 21:17:23.247777 28539 net.cpp:106] Creating Layer ip2
I0210 21:17:23.247787 28539 net.cpp:454] ip2 <- ip1
I0210 21:17:23.247805 28539 net.cpp:411] ip2 -> ip2
I0210 21:17:23.248078 28539 net.cpp:150] Setting up ip2
I0210 21:17:23.248098 28539 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:23.248108 28539 net.cpp:165] Memory required for data: 8078800
I0210 21:17:23.248123 28539 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:17:23.248138 28539 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:17:23.248148 28539 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:17:23.248162 28539 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:17:23.248177 28539 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:17:23.248244 28539 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:17:23.248260 28539 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:23.248271 28539 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:17:23.248281 28539 net.cpp:165] Memory required for data: 8086800
I0210 21:17:23.248291 28539 layer_factory.hpp:77] Creating layer accuracy
I0210 21:17:23.248316 28539 net.cpp:106] Creating Layer accuracy
I0210 21:17:23.248327 28539 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:17:23.248338 28539 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:17:23.248353 28539 net.cpp:411] accuracy -> accuracy
I0210 21:17:23.248375 28539 net.cpp:150] Setting up accuracy
I0210 21:17:23.248389 28539 net.cpp:157] Top shape: (1)
I0210 21:17:23.248399 28539 net.cpp:165] Memory required for data: 8086804
I0210 21:17:23.248409 28539 layer_factory.hpp:77] Creating layer loss
I0210 21:17:23.248425 28539 net.cpp:106] Creating Layer loss
I0210 21:17:23.248436 28539 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:17:23.248447 28539 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:17:23.248458 28539 net.cpp:411] loss -> loss
I0210 21:17:23.248474 28539 layer_factory.hpp:77] Creating layer loss
I0210 21:17:23.248663 28539 net.cpp:150] Setting up loss
I0210 21:17:23.248683 28539 net.cpp:157] Top shape: (1)
I0210 21:17:23.248698 28539 net.cpp:160]     with loss weight 1
I0210 21:17:23.248718 28539 net.cpp:165] Memory required for data: 8086808
I0210 21:17:23.248728 28539 net.cpp:226] loss needs backward computation.
I0210 21:17:23.248739 28539 net.cpp:228] accuracy does not need backward computation.
I0210 21:17:23.248754 28539 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:17:23.248764 28539 net.cpp:226] ip2 needs backward computation.
I0210 21:17:23.248775 28539 net.cpp:226] relu1 needs backward computation.
I0210 21:17:23.248783 28539 net.cpp:226] ip1 needs backward computation.
I0210 21:17:23.248793 28539 net.cpp:226] pool2 needs backward computation.
I0210 21:17:23.248805 28539 net.cpp:226] conv2 needs backward computation.
I0210 21:17:23.248817 28539 net.cpp:226] pool1 needs backward computation.
I0210 21:17:23.248826 28539 net.cpp:226] conv1 needs backward computation.
I0210 21:17:23.248836 28539 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:17:23.248847 28539 net.cpp:228] mnist does not need backward computation.
I0210 21:17:23.248855 28539 net.cpp:270] This network produces output accuracy
I0210 21:17:23.248867 28539 net.cpp:270] This network produces output loss
I0210 21:17:23.248911 28539 net.cpp:283] Network initialization done.
I0210 21:17:23.248988 28539 solver.cpp:60] Solver scaffolding done.
I0210 21:17:23.249466 28539 caffe.cpp:212] Starting Optimization
I0210 21:17:23.249482 28539 solver.cpp:311] Solving LeNet
I0210 21:17:23.249490 28539 solver.cpp:312] Learning Rate Policy: inv
I0210 21:17:23.250305 28539 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:17:24.203158 28539 solver.cpp:432]     Test net output #0: accuracy = 0.095
I0210 21:17:24.203203 28539 solver.cpp:432]     Test net output #1: loss = 2.37932 (* 1 = 2.37932 loss)
I0210 21:17:24.215319 28539 solver.cpp:250] Iteration 0, loss = 2.36163 Time spent communicating 0.002976
I0210 21:17:24.215349 28539 solver.cpp:267]     Train net output #0: loss = 2.36163 (* 1 = 2.36163 loss)
I0210 21:17:24.219630 28539 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:17:25.724452 28539 solver.cpp:250] Iteration 100, loss = 0.208216 Time spent communicating 0.43392
I0210 21:17:25.724501 28539 solver.cpp:267]     Train net output #0: loss = 0.208216 (* 1 = 0.208216 loss)
I0210 21:17:25.729015 28539 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:17:27.233474 28539 solver.cpp:250] Iteration 200, loss = 0.143027 Time spent communicating 0.434048
I0210 21:17:27.233521 28539 solver.cpp:267]     Train net output #0: loss = 0.143027 (* 1 = 0.143027 loss)
I0210 21:17:27.238013 28539 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:17:28.743103 28539 solver.cpp:250] Iteration 300, loss = 0.15529 Time spent communicating 0.438368
I0210 21:17:28.743145 28539 solver.cpp:267]     Train net output #0: loss = 0.15529 (* 1 = 0.15529 loss)
I0210 21:17:28.747666 28539 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:17:30.262699 28539 solver.cpp:250] Iteration 400, loss = 0.0748266 Time spent communicating 0.485472
I0210 21:17:30.262751 28539 solver.cpp:267]     Train net output #0: loss = 0.0748266 (* 1 = 0.0748266 loss)
I0210 21:17:30.268451 28539 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:17:31.765029 28539 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:17:31.796310 28539 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:17:31.829896 28539 solver.cpp:344] Iteration 500, loss = 0.099573
I0210 21:17:31.829960 28539 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:17:32.773558 28539 solver.cpp:432]     Test net output #0: accuracy = 0.9726
I0210 21:17:32.773607 28539 solver.cpp:432]     Test net output #1: loss = 0.0897518 (* 1 = 0.0897518 loss)
I0210 21:17:32.773617 28539 solver.cpp:349] Optimization Done.
I0210 21:17:32.773622 28539 caffe.cpp:215] Optimization Done.
I0210 21:18:27.540603 28575 caffe.cpp:184] Using GPUs 1
I0210 21:18:28.022644 28575 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 1
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:18:28.032572 28575 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:18:28.034071 28575 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:18:28.034096 28575 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:18:28.034240 28575 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:18:28.034334 28575 layer_factory.hpp:77] Creating layer mnist
I0210 21:18:28.035660 28575 net.cpp:106] Creating Layer mnist
I0210 21:18:28.035681 28575 net.cpp:411] mnist -> data
I0210 21:18:28.035812 28575 net.cpp:411] mnist -> label
I0210 21:18:28.040982 28579 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:18:28.058964 28575 data_layer.cpp:41] output data size: 64,1,28,28
I0210 21:18:28.060502 28575 net.cpp:150] Setting up mnist
I0210 21:18:28.060526 28575 net.cpp:157] Top shape: 64 1 28 28 (50176)
I0210 21:18:28.060539 28575 net.cpp:157] Top shape: 64 (64)
I0210 21:18:28.060545 28575 net.cpp:165] Memory required for data: 200960
I0210 21:18:28.060566 28575 layer_factory.hpp:77] Creating layer conv1
I0210 21:18:28.060602 28575 net.cpp:106] Creating Layer conv1
I0210 21:18:28.060613 28575 net.cpp:454] conv1 <- data
I0210 21:18:28.060641 28575 net.cpp:411] conv1 -> conv1
I0210 21:18:28.062098 28575 net.cpp:150] Setting up conv1
I0210 21:18:28.062114 28575 net.cpp:157] Top shape: 64 20 24 24 (737280)
I0210 21:18:28.062117 28575 net.cpp:165] Memory required for data: 3150080
I0210 21:18:28.062136 28575 layer_factory.hpp:77] Creating layer pool1
I0210 21:18:28.062161 28575 net.cpp:106] Creating Layer pool1
I0210 21:18:28.062168 28575 net.cpp:454] pool1 <- conv1
I0210 21:18:28.062176 28575 net.cpp:411] pool1 -> pool1
I0210 21:18:28.062388 28575 net.cpp:150] Setting up pool1
I0210 21:18:28.062400 28575 net.cpp:157] Top shape: 64 20 12 12 (184320)
I0210 21:18:28.062405 28575 net.cpp:165] Memory required for data: 3887360
I0210 21:18:28.062412 28575 layer_factory.hpp:77] Creating layer conv2
I0210 21:18:28.062430 28575 net.cpp:106] Creating Layer conv2
I0210 21:18:28.062438 28575 net.cpp:454] conv2 <- pool1
I0210 21:18:28.062449 28575 net.cpp:411] conv2 -> conv2
I0210 21:18:28.062877 28575 net.cpp:150] Setting up conv2
I0210 21:18:28.062904 28575 net.cpp:157] Top shape: 64 50 8 8 (204800)
I0210 21:18:28.062911 28575 net.cpp:165] Memory required for data: 4706560
I0210 21:18:28.062923 28575 layer_factory.hpp:77] Creating layer pool2
I0210 21:18:28.062939 28575 net.cpp:106] Creating Layer pool2
I0210 21:18:28.062945 28575 net.cpp:454] pool2 <- conv2
I0210 21:18:28.062952 28575 net.cpp:411] pool2 -> pool2
I0210 21:18:28.063129 28575 net.cpp:150] Setting up pool2
I0210 21:18:28.063143 28575 net.cpp:157] Top shape: 64 50 4 4 (51200)
I0210 21:18:28.063148 28575 net.cpp:165] Memory required for data: 4911360
I0210 21:18:28.063153 28575 layer_factory.hpp:77] Creating layer ip1
I0210 21:18:28.063174 28575 net.cpp:106] Creating Layer ip1
I0210 21:18:28.063180 28575 net.cpp:454] ip1 <- pool2
I0210 21:18:28.063189 28575 net.cpp:411] ip1 -> ip1
I0210 21:18:28.068609 28575 net.cpp:150] Setting up ip1
I0210 21:18:28.068625 28575 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:18:28.068630 28575 net.cpp:165] Memory required for data: 5039360
I0210 21:18:28.068644 28575 layer_factory.hpp:77] Creating layer relu1
I0210 21:18:28.068655 28575 net.cpp:106] Creating Layer relu1
I0210 21:18:28.068661 28575 net.cpp:454] relu1 <- ip1
I0210 21:18:28.068670 28575 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:18:28.068682 28575 net.cpp:150] Setting up relu1
I0210 21:18:28.068691 28575 net.cpp:157] Top shape: 64 500 (32000)
I0210 21:18:28.068696 28575 net.cpp:165] Memory required for data: 5167360
I0210 21:18:28.068701 28575 layer_factory.hpp:77] Creating layer ip2
I0210 21:18:28.068714 28575 net.cpp:106] Creating Layer ip2
I0210 21:18:28.068720 28575 net.cpp:454] ip2 <- ip1
I0210 21:18:28.068728 28575 net.cpp:411] ip2 -> ip2
I0210 21:18:28.069828 28575 net.cpp:150] Setting up ip2
I0210 21:18:28.069844 28575 net.cpp:157] Top shape: 64 10 (640)
I0210 21:18:28.069847 28575 net.cpp:165] Memory required for data: 5169920
I0210 21:18:28.069855 28575 layer_factory.hpp:77] Creating layer loss
I0210 21:18:28.069869 28575 net.cpp:106] Creating Layer loss
I0210 21:18:28.069875 28575 net.cpp:454] loss <- ip2
I0210 21:18:28.069880 28575 net.cpp:454] loss <- label
I0210 21:18:28.069896 28575 net.cpp:411] loss -> loss
I0210 21:18:28.069919 28575 layer_factory.hpp:77] Creating layer loss
I0210 21:18:28.070024 28575 net.cpp:150] Setting up loss
I0210 21:18:28.070044 28575 net.cpp:157] Top shape: (1)
I0210 21:18:28.070050 28575 net.cpp:160]     with loss weight 1
I0210 21:18:28.070082 28575 net.cpp:165] Memory required for data: 5169924
I0210 21:18:28.070091 28575 net.cpp:226] loss needs backward computation.
I0210 21:18:28.070097 28575 net.cpp:226] ip2 needs backward computation.
I0210 21:18:28.070103 28575 net.cpp:226] relu1 needs backward computation.
I0210 21:18:28.070107 28575 net.cpp:226] ip1 needs backward computation.
I0210 21:18:28.070112 28575 net.cpp:226] pool2 needs backward computation.
I0210 21:18:28.070117 28575 net.cpp:226] conv2 needs backward computation.
I0210 21:18:28.070127 28575 net.cpp:226] pool1 needs backward computation.
I0210 21:18:28.070132 28575 net.cpp:226] conv1 needs backward computation.
I0210 21:18:28.070138 28575 net.cpp:228] mnist does not need backward computation.
I0210 21:18:28.070143 28575 net.cpp:270] This network produces output loss
I0210 21:18:28.070158 28575 net.cpp:283] Network initialization done.
I0210 21:18:28.071354 28575 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:18:28.071403 28575 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:18:28.071605 28575 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:18:28.071720 28575 layer_factory.hpp:77] Creating layer mnist
I0210 21:18:28.071935 28575 net.cpp:106] Creating Layer mnist
I0210 21:18:28.071949 28575 net.cpp:411] mnist -> data
I0210 21:18:28.071961 28575 net.cpp:411] mnist -> label
I0210 21:18:28.077325 28581 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:18:28.077878 28575 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:18:28.079730 28575 net.cpp:150] Setting up mnist
I0210 21:18:28.079748 28575 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:18:28.079757 28575 net.cpp:157] Top shape: 100 (100)
I0210 21:18:28.079763 28575 net.cpp:165] Memory required for data: 314000
I0210 21:18:28.079782 28575 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:18:28.079815 28575 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:18:28.079823 28575 net.cpp:454] label_mnist_1_split <- label
I0210 21:18:28.079846 28575 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:18:28.079859 28575 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:18:28.079980 28575 net.cpp:150] Setting up label_mnist_1_split
I0210 21:18:28.079993 28575 net.cpp:157] Top shape: 100 (100)
I0210 21:18:28.080000 28575 net.cpp:157] Top shape: 100 (100)
I0210 21:18:28.080005 28575 net.cpp:165] Memory required for data: 314800
I0210 21:18:28.080013 28575 layer_factory.hpp:77] Creating layer conv1
I0210 21:18:28.080044 28575 net.cpp:106] Creating Layer conv1
I0210 21:18:28.080051 28575 net.cpp:454] conv1 <- data
I0210 21:18:28.080065 28575 net.cpp:411] conv1 -> conv1
I0210 21:18:28.080415 28575 net.cpp:150] Setting up conv1
I0210 21:18:28.080427 28575 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:18:28.080433 28575 net.cpp:165] Memory required for data: 4922800
I0210 21:18:28.080446 28575 layer_factory.hpp:77] Creating layer pool1
I0210 21:18:28.080461 28575 net.cpp:106] Creating Layer pool1
I0210 21:18:28.080467 28575 net.cpp:454] pool1 <- conv1
I0210 21:18:28.080494 28575 net.cpp:411] pool1 -> pool1
I0210 21:18:28.080677 28575 net.cpp:150] Setting up pool1
I0210 21:18:28.080693 28575 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:18:28.080699 28575 net.cpp:165] Memory required for data: 6074800
I0210 21:18:28.080705 28575 layer_factory.hpp:77] Creating layer conv2
I0210 21:18:28.080731 28575 net.cpp:106] Creating Layer conv2
I0210 21:18:28.080739 28575 net.cpp:454] conv2 <- pool1
I0210 21:18:28.080754 28575 net.cpp:411] conv2 -> conv2
I0210 21:18:28.081218 28575 net.cpp:150] Setting up conv2
I0210 21:18:28.081230 28575 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:18:28.081236 28575 net.cpp:165] Memory required for data: 7354800
I0210 21:18:28.081248 28575 layer_factory.hpp:77] Creating layer pool2
I0210 21:18:28.081271 28575 net.cpp:106] Creating Layer pool2
I0210 21:18:28.081279 28575 net.cpp:454] pool2 <- conv2
I0210 21:18:28.081286 28575 net.cpp:411] pool2 -> pool2
I0210 21:18:28.081473 28575 net.cpp:150] Setting up pool2
I0210 21:18:28.081485 28575 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:18:28.081491 28575 net.cpp:165] Memory required for data: 7674800
I0210 21:18:28.081496 28575 layer_factory.hpp:77] Creating layer ip1
I0210 21:18:28.081511 28575 net.cpp:106] Creating Layer ip1
I0210 21:18:28.081534 28575 net.cpp:454] ip1 <- pool2
I0210 21:18:28.081548 28575 net.cpp:411] ip1 -> ip1
I0210 21:18:28.087030 28575 net.cpp:150] Setting up ip1
I0210 21:18:28.087045 28575 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:18:28.087054 28575 net.cpp:165] Memory required for data: 7874800
I0210 21:18:28.087066 28575 layer_factory.hpp:77] Creating layer relu1
I0210 21:18:28.087077 28575 net.cpp:106] Creating Layer relu1
I0210 21:18:28.087088 28575 net.cpp:454] relu1 <- ip1
I0210 21:18:28.087096 28575 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:18:28.087107 28575 net.cpp:150] Setting up relu1
I0210 21:18:28.087116 28575 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:18:28.087121 28575 net.cpp:165] Memory required for data: 8074800
I0210 21:18:28.087126 28575 layer_factory.hpp:77] Creating layer ip2
I0210 21:18:28.087137 28575 net.cpp:106] Creating Layer ip2
I0210 21:18:28.087143 28575 net.cpp:454] ip2 <- ip1
I0210 21:18:28.087155 28575 net.cpp:411] ip2 -> ip2
I0210 21:18:28.087324 28575 net.cpp:150] Setting up ip2
I0210 21:18:28.087335 28575 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:28.087342 28575 net.cpp:165] Memory required for data: 8078800
I0210 21:18:28.087350 28575 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:18:28.087359 28575 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:18:28.087365 28575 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:18:28.087373 28575 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:18:28.087383 28575 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:18:28.087425 28575 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:18:28.087436 28575 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:28.087445 28575 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:18:28.087450 28575 net.cpp:165] Memory required for data: 8086800
I0210 21:18:28.087455 28575 layer_factory.hpp:77] Creating layer accuracy
I0210 21:18:28.087467 28575 net.cpp:106] Creating Layer accuracy
I0210 21:18:28.087473 28575 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:18:28.087481 28575 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:18:28.087487 28575 net.cpp:411] accuracy -> accuracy
I0210 21:18:28.087502 28575 net.cpp:150] Setting up accuracy
I0210 21:18:28.087512 28575 net.cpp:157] Top shape: (1)
I0210 21:18:28.087515 28575 net.cpp:165] Memory required for data: 8086804
I0210 21:18:28.087520 28575 layer_factory.hpp:77] Creating layer loss
I0210 21:18:28.087532 28575 net.cpp:106] Creating Layer loss
I0210 21:18:28.087538 28575 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:18:28.087546 28575 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:18:28.087554 28575 net.cpp:411] loss -> loss
I0210 21:18:28.087565 28575 layer_factory.hpp:77] Creating layer loss
I0210 21:18:28.087687 28575 net.cpp:150] Setting up loss
I0210 21:18:28.087702 28575 net.cpp:157] Top shape: (1)
I0210 21:18:28.087707 28575 net.cpp:160]     with loss weight 1
I0210 21:18:28.087719 28575 net.cpp:165] Memory required for data: 8086808
I0210 21:18:28.087728 28575 net.cpp:226] loss needs backward computation.
I0210 21:18:28.087734 28575 net.cpp:228] accuracy does not need backward computation.
I0210 21:18:28.087741 28575 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:18:28.087748 28575 net.cpp:226] ip2 needs backward computation.
I0210 21:18:28.087754 28575 net.cpp:226] relu1 needs backward computation.
I0210 21:18:28.087759 28575 net.cpp:226] ip1 needs backward computation.
I0210 21:18:28.087764 28575 net.cpp:226] pool2 needs backward computation.
I0210 21:18:28.087770 28575 net.cpp:226] conv2 needs backward computation.
I0210 21:18:28.087776 28575 net.cpp:226] pool1 needs backward computation.
I0210 21:18:28.087782 28575 net.cpp:226] conv1 needs backward computation.
I0210 21:18:28.087788 28575 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:18:28.087795 28575 net.cpp:228] mnist does not need backward computation.
I0210 21:18:28.087803 28575 net.cpp:270] This network produces output accuracy
I0210 21:18:28.087808 28575 net.cpp:270] This network produces output loss
I0210 21:18:28.087832 28575 net.cpp:283] Network initialization done.
I0210 21:18:28.087882 28575 solver.cpp:60] Solver scaffolding done.
I0210 21:18:28.088176 28575 caffe.cpp:212] Starting Optimization
I0210 21:18:28.088186 28575 solver.cpp:311] Solving LeNet
I0210 21:18:28.088189 28575 solver.cpp:312] Learning Rate Policy: inv
I0210 21:18:28.088850 28575 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:18:29.042481 28575 solver.cpp:432]     Test net output #0: accuracy = 0.1022
I0210 21:18:29.042534 28575 solver.cpp:432]     Test net output #1: loss = 2.30454 (* 1 = 2.30454 loss)
I0210 21:18:29.055325 28575 solver.cpp:250] Iteration 0, loss = 2.25199 Time spent communicating 0.002912
I0210 21:18:29.055367 28575 solver.cpp:267]     Train net output #0: loss = 2.25199 (* 1 = 2.25199 loss)
I0210 21:18:29.059286 28575 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:18:30.564363 28575 solver.cpp:250] Iteration 100, loss = 0.215836 Time spent communicating 0.432352
I0210 21:18:30.564414 28575 solver.cpp:267]     Train net output #0: loss = 0.215836 (* 1 = 0.215836 loss)
I0210 21:18:30.568578 28575 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:18:32.069936 28575 solver.cpp:250] Iteration 200, loss = 0.146003 Time spent communicating 0.43424
I0210 21:18:32.069983 28575 solver.cpp:267]     Train net output #0: loss = 0.146003 (* 1 = 0.146003 loss)
I0210 21:18:32.074169 28575 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:18:33.576982 28575 solver.cpp:250] Iteration 300, loss = 0.167726 Time spent communicating 0.438432
I0210 21:18:33.577028 28575 solver.cpp:267]     Train net output #0: loss = 0.167726 (* 1 = 0.167726 loss)
I0210 21:18:33.581462 28575 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:18:35.084108 28575 solver.cpp:250] Iteration 400, loss = 0.0738484 Time spent communicating 0.431008
I0210 21:18:35.084156 28575 solver.cpp:267]     Train net output #0: loss = 0.0738484 (* 1 = 0.0738484 loss)
I0210 21:18:35.088291 28575 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:18:36.579581 28575 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:18:36.609870 28575 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:18:36.640350 28575 solver.cpp:344] Iteration 500, loss = 0.0936921
I0210 21:18:36.640388 28575 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:18:37.594400 28575 solver.cpp:432]     Test net output #0: accuracy = 0.9723
I0210 21:18:37.594454 28575 solver.cpp:432]     Test net output #1: loss = 0.087442 (* 1 = 0.087442 loss)
I0210 21:18:37.594461 28575 solver.cpp:349] Optimization Done.
I0210 21:18:37.594467 28575 caffe.cpp:215] Optimization Done.
