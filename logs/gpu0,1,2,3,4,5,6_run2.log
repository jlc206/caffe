I0211 09:37:26.508455  4519 caffe.cpp:184] Using GPUs 0, 1, 2, 3, 4, 5, 6
I0211 09:37:31.290971  4519 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
I0211 09:37:31.300392  4519 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0211 09:37:31.301633  4519 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0211 09:37:31.301690  4519 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0211 09:37:31.301986  4519 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 18
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:37:31.302088  4519 layer_factory.hpp:77] Creating layer mnist
I0211 09:37:31.303234  4519 net.cpp:106] Creating Layer mnist
I0211 09:37:31.303254  4519 net.cpp:411] mnist -> data
I0211 09:37:31.303290  4519 net.cpp:411] mnist -> label
I0211 09:37:31.308976  4523 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0211 09:37:31.328337  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:31.329421  4519 net.cpp:150] Setting up mnist
I0211 09:37:31.329442  4519 net.cpp:157] Top shape: 18 1 28 28 (14112)
I0211 09:37:31.329450  4519 net.cpp:157] Top shape: 18 (18)
I0211 09:37:31.329457  4519 net.cpp:165] Memory required for data: 56520
I0211 09:37:31.329470  4519 layer_factory.hpp:77] Creating layer conv1
I0211 09:37:31.329510  4519 net.cpp:106] Creating Layer conv1
I0211 09:37:31.329522  4519 net.cpp:454] conv1 <- data
I0211 09:37:31.329542  4519 net.cpp:411] conv1 -> conv1
I0211 09:37:31.330802  4519 net.cpp:150] Setting up conv1
I0211 09:37:31.330817  4519 net.cpp:157] Top shape: 18 20 24 24 (207360)
I0211 09:37:31.330822  4519 net.cpp:165] Memory required for data: 885960
I0211 09:37:31.330839  4519 layer_factory.hpp:77] Creating layer pool1
I0211 09:37:31.330857  4519 net.cpp:106] Creating Layer pool1
I0211 09:37:31.330863  4519 net.cpp:454] pool1 <- conv1
I0211 09:37:31.330874  4519 net.cpp:411] pool1 -> pool1
I0211 09:37:31.331084  4519 net.cpp:150] Setting up pool1
I0211 09:37:31.331097  4519 net.cpp:157] Top shape: 18 20 12 12 (51840)
I0211 09:37:31.331102  4519 net.cpp:165] Memory required for data: 1093320
I0211 09:37:31.331109  4519 layer_factory.hpp:77] Creating layer conv2
I0211 09:37:31.331123  4519 net.cpp:106] Creating Layer conv2
I0211 09:37:31.331130  4519 net.cpp:454] conv2 <- pool1
I0211 09:37:31.331138  4519 net.cpp:411] conv2 -> conv2
I0211 09:37:31.332469  4519 net.cpp:150] Setting up conv2
I0211 09:37:31.332486  4519 net.cpp:157] Top shape: 18 50 8 8 (57600)
I0211 09:37:31.332491  4519 net.cpp:165] Memory required for data: 1323720
I0211 09:37:31.332502  4519 layer_factory.hpp:77] Creating layer pool2
I0211 09:37:31.332512  4519 net.cpp:106] Creating Layer pool2
I0211 09:37:31.332520  4519 net.cpp:454] pool2 <- conv2
I0211 09:37:31.332526  4519 net.cpp:411] pool2 -> pool2
I0211 09:37:31.332674  4519 net.cpp:150] Setting up pool2
I0211 09:37:31.332685  4519 net.cpp:157] Top shape: 18 50 4 4 (14400)
I0211 09:37:31.332690  4519 net.cpp:165] Memory required for data: 1381320
I0211 09:37:31.332696  4519 layer_factory.hpp:77] Creating layer ip1
I0211 09:37:31.332715  4519 net.cpp:106] Creating Layer ip1
I0211 09:37:31.332722  4519 net.cpp:454] ip1 <- pool2
I0211 09:37:31.332734  4519 net.cpp:411] ip1 -> ip1
I0211 09:37:31.334080  4524 blocking_queue.cpp:50] Waiting for data
I0211 09:37:31.337260  4519 net.cpp:150] Setting up ip1
I0211 09:37:31.337276  4519 net.cpp:157] Top shape: 18 500 (9000)
I0211 09:37:31.337280  4519 net.cpp:165] Memory required for data: 1417320
I0211 09:37:31.337292  4519 layer_factory.hpp:77] Creating layer relu1
I0211 09:37:31.337306  4519 net.cpp:106] Creating Layer relu1
I0211 09:37:31.337311  4519 net.cpp:454] relu1 <- ip1
I0211 09:37:31.337317  4519 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:37:31.337332  4519 net.cpp:150] Setting up relu1
I0211 09:37:31.337338  4519 net.cpp:157] Top shape: 18 500 (9000)
I0211 09:37:31.337343  4519 net.cpp:165] Memory required for data: 1453320
I0211 09:37:31.337348  4519 layer_factory.hpp:77] Creating layer ip2
I0211 09:37:31.337360  4519 net.cpp:106] Creating Layer ip2
I0211 09:37:31.337368  4519 net.cpp:454] ip2 <- ip1
I0211 09:37:31.337375  4519 net.cpp:411] ip2 -> ip2
I0211 09:37:31.337528  4519 net.cpp:150] Setting up ip2
I0211 09:37:31.337540  4519 net.cpp:157] Top shape: 18 10 (180)
I0211 09:37:31.337545  4519 net.cpp:165] Memory required for data: 1454040
I0211 09:37:31.337554  4519 layer_factory.hpp:77] Creating layer loss
I0211 09:37:31.337569  4519 net.cpp:106] Creating Layer loss
I0211 09:37:31.337575  4519 net.cpp:454] loss <- ip2
I0211 09:37:31.337582  4519 net.cpp:454] loss <- label
I0211 09:37:31.337591  4519 net.cpp:411] loss -> loss
I0211 09:37:31.337612  4519 layer_factory.hpp:77] Creating layer loss
I0211 09:37:31.337709  4519 net.cpp:150] Setting up loss
I0211 09:37:31.337720  4519 net.cpp:157] Top shape: (1)
I0211 09:37:31.337725  4519 net.cpp:160]     with loss weight 1
I0211 09:37:31.337754  4519 net.cpp:165] Memory required for data: 1454044
I0211 09:37:31.337764  4519 net.cpp:226] loss needs backward computation.
I0211 09:37:31.337770  4519 net.cpp:226] ip2 needs backward computation.
I0211 09:37:31.337775  4519 net.cpp:226] relu1 needs backward computation.
I0211 09:37:31.337781  4519 net.cpp:226] ip1 needs backward computation.
I0211 09:37:31.337785  4519 net.cpp:226] pool2 needs backward computation.
I0211 09:37:31.337790  4519 net.cpp:226] conv2 needs backward computation.
I0211 09:37:31.337798  4519 net.cpp:226] pool1 needs backward computation.
I0211 09:37:31.337805  4519 net.cpp:226] conv1 needs backward computation.
I0211 09:37:31.337810  4519 net.cpp:228] mnist does not need backward computation.
I0211 09:37:31.337815  4519 net.cpp:270] This network produces output loss
I0211 09:37:31.337829  4519 net.cpp:283] Network initialization done.
I0211 09:37:31.338948  4519 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0211 09:37:31.338992  4519 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0211 09:37:31.339184  4519 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0211 09:37:31.339284  4519 layer_factory.hpp:77] Creating layer mnist
I0211 09:37:31.339426  4519 net.cpp:106] Creating Layer mnist
I0211 09:37:31.339438  4519 net.cpp:411] mnist -> data
I0211 09:37:31.339452  4519 net.cpp:411] mnist -> label
I0211 09:37:31.344651  4525 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0211 09:37:31.345085  4519 data_layer.cpp:41] output data size: 100,1,28,28
I0211 09:37:31.347038  4519 net.cpp:150] Setting up mnist
I0211 09:37:31.347064  4519 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0211 09:37:31.347076  4519 net.cpp:157] Top shape: 100 (100)
I0211 09:37:31.347084  4519 net.cpp:165] Memory required for data: 314000
I0211 09:37:31.347093  4519 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0211 09:37:31.347108  4519 net.cpp:106] Creating Layer label_mnist_1_split
I0211 09:37:31.347118  4519 net.cpp:454] label_mnist_1_split <- label
I0211 09:37:31.347131  4519 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0211 09:37:31.347148  4519 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0211 09:37:31.347292  4519 net.cpp:150] Setting up label_mnist_1_split
I0211 09:37:31.347314  4519 net.cpp:157] Top shape: 100 (100)
I0211 09:37:31.347324  4519 net.cpp:157] Top shape: 100 (100)
I0211 09:37:31.347333  4519 net.cpp:165] Memory required for data: 314800
I0211 09:37:31.347342  4519 layer_factory.hpp:77] Creating layer conv1
I0211 09:37:31.347362  4519 net.cpp:106] Creating Layer conv1
I0211 09:37:31.347373  4519 net.cpp:454] conv1 <- data
I0211 09:37:31.347390  4519 net.cpp:411] conv1 -> conv1
I0211 09:37:31.347767  4519 net.cpp:150] Setting up conv1
I0211 09:37:31.347787  4519 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0211 09:37:31.347795  4519 net.cpp:165] Memory required for data: 4922800
I0211 09:37:31.347813  4519 layer_factory.hpp:77] Creating layer pool1
I0211 09:37:31.347832  4519 net.cpp:106] Creating Layer pool1
I0211 09:37:31.347865  4519 net.cpp:454] pool1 <- conv1
I0211 09:37:31.347887  4519 net.cpp:411] pool1 -> pool1
I0211 09:37:31.348127  4519 net.cpp:150] Setting up pool1
I0211 09:37:31.348145  4519 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0211 09:37:31.348152  4519 net.cpp:165] Memory required for data: 6074800
I0211 09:37:31.348161  4519 layer_factory.hpp:77] Creating layer conv2
I0211 09:37:31.348184  4519 net.cpp:106] Creating Layer conv2
I0211 09:37:31.348194  4519 net.cpp:454] conv2 <- pool1
I0211 09:37:31.348208  4519 net.cpp:411] conv2 -> conv2
I0211 09:37:31.348860  4519 net.cpp:150] Setting up conv2
I0211 09:37:31.348880  4519 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0211 09:37:31.348886  4519 net.cpp:165] Memory required for data: 7354800
I0211 09:37:31.348912  4519 layer_factory.hpp:77] Creating layer pool2
I0211 09:37:31.348927  4519 net.cpp:106] Creating Layer pool2
I0211 09:37:31.348940  4519 net.cpp:454] pool2 <- conv2
I0211 09:37:31.348953  4519 net.cpp:411] pool2 -> pool2
I0211 09:37:31.349179  4519 net.cpp:150] Setting up pool2
I0211 09:37:31.349196  4519 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0211 09:37:31.349205  4519 net.cpp:165] Memory required for data: 7674800
I0211 09:37:31.349213  4519 layer_factory.hpp:77] Creating layer ip1
I0211 09:37:31.349228  4519 net.cpp:106] Creating Layer ip1
I0211 09:37:31.349237  4519 net.cpp:454] ip1 <- pool2
I0211 09:37:31.349253  4519 net.cpp:411] ip1 -> ip1
I0211 09:37:31.356362  4519 net.cpp:150] Setting up ip1
I0211 09:37:31.356392  4519 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:37:31.356402  4519 net.cpp:165] Memory required for data: 7874800
I0211 09:37:31.356421  4519 layer_factory.hpp:77] Creating layer relu1
I0211 09:37:31.356437  4519 net.cpp:106] Creating Layer relu1
I0211 09:37:31.356448  4519 net.cpp:454] relu1 <- ip1
I0211 09:37:31.356462  4519 net.cpp:397] relu1 -> ip1 (in-place)
I0211 09:37:31.356478  4519 net.cpp:150] Setting up relu1
I0211 09:37:31.356490  4519 net.cpp:157] Top shape: 100 500 (50000)
I0211 09:37:31.356499  4519 net.cpp:165] Memory required for data: 8074800
I0211 09:37:31.356509  4519 layer_factory.hpp:77] Creating layer ip2
I0211 09:37:31.356531  4519 net.cpp:106] Creating Layer ip2
I0211 09:37:31.356542  4519 net.cpp:454] ip2 <- ip1
I0211 09:37:31.356556  4519 net.cpp:411] ip2 -> ip2
I0211 09:37:31.356824  4519 net.cpp:150] Setting up ip2
I0211 09:37:31.356844  4519 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:31.356854  4519 net.cpp:165] Memory required for data: 8078800
I0211 09:37:31.356868  4519 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0211 09:37:31.356881  4519 net.cpp:106] Creating Layer ip2_ip2_0_split
I0211 09:37:31.356899  4519 net.cpp:454] ip2_ip2_0_split <- ip2
I0211 09:37:31.356912  4519 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0211 09:37:31.356928  4519 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0211 09:37:31.357002  4519 net.cpp:150] Setting up ip2_ip2_0_split
I0211 09:37:31.357018  4519 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:31.357030  4519 net.cpp:157] Top shape: 100 10 (1000)
I0211 09:37:31.357039  4519 net.cpp:165] Memory required for data: 8086800
I0211 09:37:31.357048  4519 layer_factory.hpp:77] Creating layer accuracy
I0211 09:37:31.357070  4519 net.cpp:106] Creating Layer accuracy
I0211 09:37:31.357080  4519 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0211 09:37:31.357094  4519 net.cpp:454] accuracy <- label_mnist_1_split_0
I0211 09:37:31.357111  4519 net.cpp:411] accuracy -> accuracy
I0211 09:37:31.357136  4519 net.cpp:150] Setting up accuracy
I0211 09:37:31.357151  4519 net.cpp:157] Top shape: (1)
I0211 09:37:31.357158  4519 net.cpp:165] Memory required for data: 8086804
I0211 09:37:31.357166  4519 layer_factory.hpp:77] Creating layer loss
I0211 09:37:31.357177  4519 net.cpp:106] Creating Layer loss
I0211 09:37:31.357188  4519 net.cpp:454] loss <- ip2_ip2_0_split_1
I0211 09:37:31.357199  4519 net.cpp:454] loss <- label_mnist_1_split_1
I0211 09:37:31.357216  4519 net.cpp:411] loss -> loss
I0211 09:37:31.357261  4519 layer_factory.hpp:77] Creating layer loss
I0211 09:37:31.357414  4519 net.cpp:150] Setting up loss
I0211 09:37:31.357432  4519 net.cpp:157] Top shape: (1)
I0211 09:37:31.357441  4519 net.cpp:160]     with loss weight 1
I0211 09:37:31.357457  4519 net.cpp:165] Memory required for data: 8086808
I0211 09:37:31.357467  4519 net.cpp:226] loss needs backward computation.
I0211 09:37:31.357478  4519 net.cpp:228] accuracy does not need backward computation.
I0211 09:37:31.357489  4519 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0211 09:37:31.357498  4519 net.cpp:226] ip2 needs backward computation.
I0211 09:37:31.357508  4519 net.cpp:226] relu1 needs backward computation.
I0211 09:37:31.357517  4519 net.cpp:226] ip1 needs backward computation.
I0211 09:37:31.357527  4519 net.cpp:226] pool2 needs backward computation.
I0211 09:37:31.357540  4519 net.cpp:226] conv2 needs backward computation.
I0211 09:37:31.357550  4519 net.cpp:226] pool1 needs backward computation.
I0211 09:37:31.357559  4519 net.cpp:226] conv1 needs backward computation.
I0211 09:37:31.357568  4519 net.cpp:228] label_mnist_1_split does not need backward computation.
I0211 09:37:31.357579  4519 net.cpp:228] mnist does not need backward computation.
I0211 09:37:31.357588  4519 net.cpp:270] This network produces output accuracy
I0211 09:37:31.357596  4519 net.cpp:270] This network produces output loss
I0211 09:37:31.357620  4519 net.cpp:283] Network initialization done.
I0211 09:37:31.357687  4519 solver.cpp:60] Solver scaffolding done.
I0211 09:37:31.476394  4519 parallel.cpp:405] GPUs pairs 0:1, 2:3, 4:5, 0:2, 4:6, 0:4
I0211 09:37:31.697630  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:32.050176  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:32.428974  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:32.690291  4519 parallel.cpp:234] GPU 4 does not have p2p access to GPU 0
I0211 09:37:32.948905  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:33.421926  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:33.924486  4519 data_layer.cpp:41] output data size: 18,1,28,28
I0211 09:37:34.212076  4519 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0211 09:37:34.214427  4519 solver.cpp:311] Solving LeNet
I0211 09:37:34.214444  4519 solver.cpp:312] Learning Rate Policy: inv
I0211 09:37:34.215102  4519 solver.cpp:364] Iteration 0, Testing net (#0)
I0211 09:37:35.217645  4519 solver.cpp:432]     Test net output #0: accuracy = 0.0915
I0211 09:37:35.217684  4519 solver.cpp:432]     Test net output #1: loss = 2.34089 (* 1 = 2.34089 loss)
I0211 09:37:35.239768  4519 solver.cpp:250] Iteration 0, loss = 2.2476 Time spent communicating 0.635328
I0211 09:37:35.239821  4519 solver.cpp:267]     Train net output #0: loss = 2.2476 (* 1 = 2.2476 loss)
I0211 09:37:35.246255  4519 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0211 09:37:36.298779  4519 solver.cpp:250] Iteration 100, loss = 0.527407 Time spent communicating 235.499
I0211 09:37:36.298842  4519 solver.cpp:267]     Train net output #0: loss = 0.527407 (* 1 = 0.527407 loss)
I0211 09:37:36.300818  4519 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0211 09:37:37.288231  4519 solver.cpp:250] Iteration 200, loss = 0.0601828 Time spent communicating 236.809
I0211 09:37:37.288275  4519 solver.cpp:267]     Train net output #0: loss = 0.0601829 (* 1 = 0.0601829 loss)
I0211 09:37:37.290367  4519 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0211 09:37:38.281081  4519 solver.cpp:250] Iteration 300, loss = 0.0442622 Time spent communicating 198.378
I0211 09:37:38.281123  4519 solver.cpp:267]     Train net output #0: loss = 0.0442624 (* 1 = 0.0442624 loss)
I0211 09:37:38.282282  4519 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0211 09:37:39.231928  4519 solver.cpp:250] Iteration 400, loss = 0.175114 Time spent communicating 207.493
I0211 09:37:39.231964  4519 solver.cpp:267]     Train net output #0: loss = 0.175114 (* 1 = 0.175114 loss)
I0211 09:37:39.233157  4519 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0211 09:37:40.221802  4519 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0211 09:37:40.258857  4519 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0211 09:37:40.286362  4519 solver.cpp:344] Iteration 500, loss = 0.0721996
I0211 09:37:40.286389  4519 solver.cpp:364] Iteration 500, Testing net (#0)
I0211 09:37:41.240734  4519 solver.cpp:432]     Test net output #0: accuracy = 0.9778
I0211 09:37:41.240767  4519 solver.cpp:432]     Test net output #1: loss = 0.0729849 (* 1 = 0.0729849 loss)
I0211 09:37:41.240774  4519 solver.cpp:349] Optimization Done.
I0211 09:37:41.240898  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 1
I0211 09:37:41.265516  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0211 09:37:41.287864  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 5
I0211 09:37:41.310354  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 2
I0211 09:37:41.333112  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 6
I0211 09:37:41.354708  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0211 09:37:41.374481  4519 parallel.cpp:256] IN DESTRUCTOR AND I'M 0
I0211 09:37:41.374928  4519 caffe.cpp:215] Optimization Done.
