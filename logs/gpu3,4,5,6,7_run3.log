I0210 21:27:01.238420 28998 caffe.cpp:184] Using GPUs 3, 4, 5, 6, 7
I0210 21:27:01.727895 28998 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 500
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 3
net: "examples/mnist/lenet_train_test.prototxt"
I0210 21:27:01.740272 28998 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0210 21:27:01.741982 28998 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0210 21:27:01.742045 28998 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0210 21:27:01.742333 28998 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 26
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:27:01.742511 28998 layer_factory.hpp:77] Creating layer mnist
I0210 21:27:01.744053 28998 net.cpp:106] Creating Layer mnist
I0210 21:27:01.744125 28998 net.cpp:411] mnist -> data
I0210 21:27:01.744197 28998 net.cpp:411] mnist -> label
I0210 21:27:01.749308 29002 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0210 21:27:01.771040 28998 data_layer.cpp:41] output data size: 26,1,28,28
I0210 21:27:01.772325 28998 net.cpp:150] Setting up mnist
I0210 21:27:01.772364 28998 net.cpp:157] Top shape: 26 1 28 28 (20384)
I0210 21:27:01.772374 28998 net.cpp:157] Top shape: 26 (26)
I0210 21:27:01.772380 28998 net.cpp:165] Memory required for data: 81640
I0210 21:27:01.772395 28998 layer_factory.hpp:77] Creating layer conv1
I0210 21:27:01.772440 28998 net.cpp:106] Creating Layer conv1
I0210 21:27:01.772459 28998 net.cpp:454] conv1 <- data
I0210 21:27:01.772490 28998 net.cpp:411] conv1 -> conv1
I0210 21:27:01.773900 28998 net.cpp:150] Setting up conv1
I0210 21:27:01.773919 28998 net.cpp:157] Top shape: 26 20 24 24 (299520)
I0210 21:27:01.773924 28998 net.cpp:165] Memory required for data: 1279720
I0210 21:27:01.773941 28998 layer_factory.hpp:77] Creating layer pool1
I0210 21:27:01.773958 28998 net.cpp:106] Creating Layer pool1
I0210 21:27:01.773965 28998 net.cpp:454] pool1 <- conv1
I0210 21:27:01.773983 28998 net.cpp:411] pool1 -> pool1
I0210 21:27:01.774188 28998 net.cpp:150] Setting up pool1
I0210 21:27:01.774199 28998 net.cpp:157] Top shape: 26 20 12 12 (74880)
I0210 21:27:01.774204 28998 net.cpp:165] Memory required for data: 1579240
I0210 21:27:01.774211 28998 layer_factory.hpp:77] Creating layer conv2
I0210 21:27:01.774229 28998 net.cpp:106] Creating Layer conv2
I0210 21:27:01.774235 28998 net.cpp:454] conv2 <- pool1
I0210 21:27:01.774245 28998 net.cpp:411] conv2 -> conv2
I0210 21:27:01.774682 28998 net.cpp:150] Setting up conv2
I0210 21:27:01.774696 28998 net.cpp:157] Top shape: 26 50 8 8 (83200)
I0210 21:27:01.774703 28998 net.cpp:165] Memory required for data: 1912040
I0210 21:27:01.774723 28998 layer_factory.hpp:77] Creating layer pool2
I0210 21:27:01.774739 28998 net.cpp:106] Creating Layer pool2
I0210 21:27:01.774751 28998 net.cpp:454] pool2 <- conv2
I0210 21:27:01.774771 28998 net.cpp:411] pool2 -> pool2
I0210 21:27:01.775012 28998 net.cpp:150] Setting up pool2
I0210 21:27:01.775025 28998 net.cpp:157] Top shape: 26 50 4 4 (20800)
I0210 21:27:01.775030 28998 net.cpp:165] Memory required for data: 1995240
I0210 21:27:01.775035 28998 layer_factory.hpp:77] Creating layer ip1
I0210 21:27:01.775054 28998 net.cpp:106] Creating Layer ip1
I0210 21:27:01.775060 28998 net.cpp:454] ip1 <- pool2
I0210 21:27:01.775070 28998 net.cpp:411] ip1 -> ip1
I0210 21:27:01.776451 29003 blocking_queue.cpp:50] Waiting for data
I0210 21:27:01.779700 28998 net.cpp:150] Setting up ip1
I0210 21:27:01.779718 28998 net.cpp:157] Top shape: 26 500 (13000)
I0210 21:27:01.779722 28998 net.cpp:165] Memory required for data: 2047240
I0210 21:27:01.779736 28998 layer_factory.hpp:77] Creating layer relu1
I0210 21:27:01.779747 28998 net.cpp:106] Creating Layer relu1
I0210 21:27:01.779752 28998 net.cpp:454] relu1 <- ip1
I0210 21:27:01.779764 28998 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:27:01.779778 28998 net.cpp:150] Setting up relu1
I0210 21:27:01.779784 28998 net.cpp:157] Top shape: 26 500 (13000)
I0210 21:27:01.779789 28998 net.cpp:165] Memory required for data: 2099240
I0210 21:27:01.779794 28998 layer_factory.hpp:77] Creating layer ip2
I0210 21:27:01.779805 28998 net.cpp:106] Creating Layer ip2
I0210 21:27:01.779811 28998 net.cpp:454] ip2 <- ip1
I0210 21:27:01.779821 28998 net.cpp:411] ip2 -> ip2
I0210 21:27:01.780709 28998 net.cpp:150] Setting up ip2
I0210 21:27:01.780724 28998 net.cpp:157] Top shape: 26 10 (260)
I0210 21:27:01.780728 28998 net.cpp:165] Memory required for data: 2100280
I0210 21:27:01.780738 28998 layer_factory.hpp:77] Creating layer loss
I0210 21:27:01.780757 28998 net.cpp:106] Creating Layer loss
I0210 21:27:01.780776 28998 net.cpp:454] loss <- ip2
I0210 21:27:01.780787 28998 net.cpp:454] loss <- label
I0210 21:27:01.780802 28998 net.cpp:411] loss -> loss
I0210 21:27:01.780834 28998 layer_factory.hpp:77] Creating layer loss
I0210 21:27:01.780967 28998 net.cpp:150] Setting up loss
I0210 21:27:01.780982 28998 net.cpp:157] Top shape: (1)
I0210 21:27:01.780987 28998 net.cpp:160]     with loss weight 1
I0210 21:27:01.781023 28998 net.cpp:165] Memory required for data: 2100284
I0210 21:27:01.781033 28998 net.cpp:226] loss needs backward computation.
I0210 21:27:01.781039 28998 net.cpp:226] ip2 needs backward computation.
I0210 21:27:01.781045 28998 net.cpp:226] relu1 needs backward computation.
I0210 21:27:01.781050 28998 net.cpp:226] ip1 needs backward computation.
I0210 21:27:01.781054 28998 net.cpp:226] pool2 needs backward computation.
I0210 21:27:01.781059 28998 net.cpp:226] conv2 needs backward computation.
I0210 21:27:01.781069 28998 net.cpp:226] pool1 needs backward computation.
I0210 21:27:01.781074 28998 net.cpp:226] conv1 needs backward computation.
I0210 21:27:01.781080 28998 net.cpp:228] mnist does not need backward computation.
I0210 21:27:01.781085 28998 net.cpp:270] This network produces output loss
I0210 21:27:01.781111 28998 net.cpp:283] Network initialization done.
I0210 21:27:01.782300 28998 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0210 21:27:01.782346 28998 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0210 21:27:01.782516 28998 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0210 21:27:01.782640 28998 layer_factory.hpp:77] Creating layer mnist
I0210 21:27:01.783334 28998 net.cpp:106] Creating Layer mnist
I0210 21:27:01.783350 28998 net.cpp:411] mnist -> data
I0210 21:27:01.783365 28998 net.cpp:411] mnist -> label
I0210 21:27:01.788208 29004 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0210 21:27:01.788516 28998 data_layer.cpp:41] output data size: 100,1,28,28
I0210 21:27:01.789988 28998 net.cpp:150] Setting up mnist
I0210 21:27:01.790004 28998 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0210 21:27:01.790011 28998 net.cpp:157] Top shape: 100 (100)
I0210 21:27:01.790017 28998 net.cpp:165] Memory required for data: 314000
I0210 21:27:01.790024 28998 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0210 21:27:01.790032 28998 net.cpp:106] Creating Layer label_mnist_1_split
I0210 21:27:01.790037 28998 net.cpp:454] label_mnist_1_split <- label
I0210 21:27:01.790048 28998 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_0
I0210 21:27:01.790060 28998 net.cpp:411] label_mnist_1_split -> label_mnist_1_split_1
I0210 21:27:01.790213 28998 net.cpp:150] Setting up label_mnist_1_split
I0210 21:27:01.790241 28998 net.cpp:157] Top shape: 100 (100)
I0210 21:27:01.790247 28998 net.cpp:157] Top shape: 100 (100)
I0210 21:27:01.790252 28998 net.cpp:165] Memory required for data: 314800
I0210 21:27:01.790259 28998 layer_factory.hpp:77] Creating layer conv1
I0210 21:27:01.790278 28998 net.cpp:106] Creating Layer conv1
I0210 21:27:01.790285 28998 net.cpp:454] conv1 <- data
I0210 21:27:01.790295 28998 net.cpp:411] conv1 -> conv1
I0210 21:27:01.790549 28998 net.cpp:150] Setting up conv1
I0210 21:27:01.790562 28998 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I0210 21:27:01.790567 28998 net.cpp:165] Memory required for data: 4922800
I0210 21:27:01.790580 28998 layer_factory.hpp:77] Creating layer pool1
I0210 21:27:01.790591 28998 net.cpp:106] Creating Layer pool1
I0210 21:27:01.790616 28998 net.cpp:454] pool1 <- conv1
I0210 21:27:01.790630 28998 net.cpp:411] pool1 -> pool1
I0210 21:27:01.790783 28998 net.cpp:150] Setting up pool1
I0210 21:27:01.790794 28998 net.cpp:157] Top shape: 100 20 12 12 (288000)
I0210 21:27:01.790799 28998 net.cpp:165] Memory required for data: 6074800
I0210 21:27:01.790804 28998 layer_factory.hpp:77] Creating layer conv2
I0210 21:27:01.790819 28998 net.cpp:106] Creating Layer conv2
I0210 21:27:01.790827 28998 net.cpp:454] conv2 <- pool1
I0210 21:27:01.790838 28998 net.cpp:411] conv2 -> conv2
I0210 21:27:01.791283 28998 net.cpp:150] Setting up conv2
I0210 21:27:01.791296 28998 net.cpp:157] Top shape: 100 50 8 8 (320000)
I0210 21:27:01.791302 28998 net.cpp:165] Memory required for data: 7354800
I0210 21:27:01.791319 28998 layer_factory.hpp:77] Creating layer pool2
I0210 21:27:01.791334 28998 net.cpp:106] Creating Layer pool2
I0210 21:27:01.791345 28998 net.cpp:454] pool2 <- conv2
I0210 21:27:01.791368 28998 net.cpp:411] pool2 -> pool2
I0210 21:27:01.791564 28998 net.cpp:150] Setting up pool2
I0210 21:27:01.791579 28998 net.cpp:157] Top shape: 100 50 4 4 (80000)
I0210 21:27:01.791584 28998 net.cpp:165] Memory required for data: 7674800
I0210 21:27:01.791589 28998 layer_factory.hpp:77] Creating layer ip1
I0210 21:27:01.791601 28998 net.cpp:106] Creating Layer ip1
I0210 21:27:01.791609 28998 net.cpp:454] ip1 <- pool2
I0210 21:27:01.791617 28998 net.cpp:411] ip1 -> ip1
I0210 21:27:01.798802 28998 net.cpp:150] Setting up ip1
I0210 21:27:01.798825 28998 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:27:01.798832 28998 net.cpp:165] Memory required for data: 7874800
I0210 21:27:01.798849 28998 layer_factory.hpp:77] Creating layer relu1
I0210 21:27:01.798864 28998 net.cpp:106] Creating Layer relu1
I0210 21:27:01.798876 28998 net.cpp:454] relu1 <- ip1
I0210 21:27:01.798887 28998 net.cpp:397] relu1 -> ip1 (in-place)
I0210 21:27:01.798908 28998 net.cpp:150] Setting up relu1
I0210 21:27:01.798919 28998 net.cpp:157] Top shape: 100 500 (50000)
I0210 21:27:01.798928 28998 net.cpp:165] Memory required for data: 8074800
I0210 21:27:01.798934 28998 layer_factory.hpp:77] Creating layer ip2
I0210 21:27:01.798952 28998 net.cpp:106] Creating Layer ip2
I0210 21:27:01.798961 28998 net.cpp:454] ip2 <- ip1
I0210 21:27:01.798974 28998 net.cpp:411] ip2 -> ip2
I0210 21:27:01.799190 28998 net.cpp:150] Setting up ip2
I0210 21:27:01.799206 28998 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:27:01.799213 28998 net.cpp:165] Memory required for data: 8078800
I0210 21:27:01.799227 28998 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0210 21:27:01.799242 28998 net.cpp:106] Creating Layer ip2_ip2_0_split
I0210 21:27:01.799252 28998 net.cpp:454] ip2_ip2_0_split <- ip2
I0210 21:27:01.799263 28998 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0210 21:27:01.799276 28998 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0210 21:27:01.799337 28998 net.cpp:150] Setting up ip2_ip2_0_split
I0210 21:27:01.799351 28998 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:27:01.799361 28998 net.cpp:157] Top shape: 100 10 (1000)
I0210 21:27:01.799370 28998 net.cpp:165] Memory required for data: 8086800
I0210 21:27:01.799377 28998 layer_factory.hpp:77] Creating layer accuracy
I0210 21:27:01.799396 28998 net.cpp:106] Creating Layer accuracy
I0210 21:27:01.799404 28998 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0210 21:27:01.799414 28998 net.cpp:454] accuracy <- label_mnist_1_split_0
I0210 21:27:01.799430 28998 net.cpp:411] accuracy -> accuracy
I0210 21:27:01.799450 28998 net.cpp:150] Setting up accuracy
I0210 21:27:01.799461 28998 net.cpp:157] Top shape: (1)
I0210 21:27:01.799469 28998 net.cpp:165] Memory required for data: 8086804
I0210 21:27:01.799475 28998 layer_factory.hpp:77] Creating layer loss
I0210 21:27:01.799486 28998 net.cpp:106] Creating Layer loss
I0210 21:27:01.799495 28998 net.cpp:454] loss <- ip2_ip2_0_split_1
I0210 21:27:01.799505 28998 net.cpp:454] loss <- label_mnist_1_split_1
I0210 21:27:01.799520 28998 net.cpp:411] loss -> loss
I0210 21:27:01.799556 28998 layer_factory.hpp:77] Creating layer loss
I0210 21:27:01.799690 28998 net.cpp:150] Setting up loss
I0210 21:27:01.799705 28998 net.cpp:157] Top shape: (1)
I0210 21:27:01.799713 28998 net.cpp:160]     with loss weight 1
I0210 21:27:01.799727 28998 net.cpp:165] Memory required for data: 8086808
I0210 21:27:01.799736 28998 net.cpp:226] loss needs backward computation.
I0210 21:27:01.799744 28998 net.cpp:228] accuracy does not need backward computation.
I0210 21:27:01.799753 28998 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0210 21:27:01.799762 28998 net.cpp:226] ip2 needs backward computation.
I0210 21:27:01.799770 28998 net.cpp:226] relu1 needs backward computation.
I0210 21:27:01.799777 28998 net.cpp:226] ip1 needs backward computation.
I0210 21:27:01.799784 28998 net.cpp:226] pool2 needs backward computation.
I0210 21:27:01.799792 28998 net.cpp:226] conv2 needs backward computation.
I0210 21:27:01.799800 28998 net.cpp:226] pool1 needs backward computation.
I0210 21:27:01.799808 28998 net.cpp:226] conv1 needs backward computation.
I0210 21:27:01.799820 28998 net.cpp:228] label_mnist_1_split does not need backward computation.
I0210 21:27:01.799830 28998 net.cpp:228] mnist does not need backward computation.
I0210 21:27:01.799837 28998 net.cpp:270] This network produces output accuracy
I0210 21:27:01.799844 28998 net.cpp:270] This network produces output loss
I0210 21:27:01.799867 28998 net.cpp:283] Network initialization done.
I0210 21:27:01.799934 28998 solver.cpp:60] Solver scaffolding done.
I0210 21:27:01.860255 28998 parallel.cpp:405] GPUs pairs 4:5, 6:7, 4:6, 3:4
I0210 21:27:02.072330 28998 data_layer.cpp:41] output data size: 26,1,28,28
I0210 21:27:02.238806 28998 parallel.cpp:234] GPU 4 does not have p2p access to GPU 3
I0210 21:27:02.436275 28998 data_layer.cpp:41] output data size: 26,1,28,28
I0210 21:27:02.845403 28998 data_layer.cpp:41] output data size: 26,1,28,28
I0210 21:27:03.239008 28998 data_layer.cpp:41] output data size: 26,1,28,28
I0210 21:27:03.359071 28998 parallel.cpp:433] Starting Optimization - TEST TEST TEST
I0210 21:27:03.359405 28998 solver.cpp:311] Solving LeNet
I0210 21:27:03.359418 28998 solver.cpp:312] Learning Rate Policy: inv
I0210 21:27:03.359647 28998 solver.cpp:364] Iteration 0, Testing net (#0)
I0210 21:27:04.647413 28998 solver.cpp:432]     Test net output #0: accuracy = 0.0961
I0210 21:27:04.647461 28998 solver.cpp:432]     Test net output #1: loss = 2.41314 (* 1 = 2.41314 loss)
I0210 21:27:04.659461 28998 solver.cpp:250] Iteration 0, loss = 2.45867 Time spent communicating 0.291904
I0210 21:27:04.659502 28998 solver.cpp:267]     Train net output #0: loss = 2.45867 (* 1 = 2.45867 loss)
I0210 21:27:04.665488 28998 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0210 21:27:05.662145 28998 solver.cpp:250] Iteration 100, loss = 0.368765 Time spent communicating 245.759
I0210 21:27:05.662194 28998 solver.cpp:267]     Train net output #0: loss = 0.368765 (* 1 = 0.368765 loss)
I0210 21:27:05.664324 28998 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0210 21:27:06.660150 28998 solver.cpp:250] Iteration 200, loss = 0.0432061 Time spent communicating 257.245
I0210 21:27:06.660179 28998 solver.cpp:267]     Train net output #0: loss = 0.0432062 (* 1 = 0.0432062 loss)
I0210 21:27:06.663156 28998 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0210 21:27:07.659579 28998 solver.cpp:250] Iteration 300, loss = 0.0721066 Time spent communicating 271.774
I0210 21:27:07.659608 28998 solver.cpp:267]     Train net output #0: loss = 0.0721068 (* 1 = 0.0721068 loss)
I0210 21:27:07.662380 28998 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0210 21:27:08.661811 28998 solver.cpp:250] Iteration 400, loss = 0.101254 Time spent communicating 268.929
I0210 21:27:08.661849 28998 solver.cpp:267]     Train net output #0: loss = 0.101254 (* 1 = 0.101254 loss)
I0210 21:27:08.663672 28998 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0210 21:27:09.650943 28998 solver.cpp:482] Snapshotting to binary proto file examples/mnist/lenet_iter_500.caffemodel
I0210 21:27:09.686113 28998 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_500.solverstate
I0210 21:27:09.714606 28998 solver.cpp:344] Iteration 500, loss = 0.0726628
I0210 21:27:09.714648 28998 solver.cpp:364] Iteration 500, Testing net (#0)
I0210 21:27:10.666177 28998 solver.cpp:432]     Test net output #0: accuracy = 0.978
I0210 21:27:10.666224 28998 solver.cpp:432]     Test net output #1: loss = 0.0731193 (* 1 = 0.0731193 loss)
I0210 21:27:10.666231 28998 solver.cpp:349] Optimization Done.
I0210 21:27:10.666337 28998 parallel.cpp:256] IN DESTRUCTOR AND I'M 5
I0210 21:27:10.686350 28998 parallel.cpp:256] IN DESTRUCTOR AND I'M 7
I0210 21:27:10.704788 28998 parallel.cpp:256] IN DESTRUCTOR AND I'M 6
I0210 21:27:10.722820 28998 parallel.cpp:256] IN DESTRUCTOR AND I'M 4
I0210 21:27:10.739192 28998 parallel.cpp:256] IN DESTRUCTOR AND I'M 3
I0210 21:27:10.739696 28998 caffe.cpp:215] Optimization Done.
